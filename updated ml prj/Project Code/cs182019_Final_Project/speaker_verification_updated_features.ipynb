{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888fb32f",
   "metadata": {},
   "source": [
    "## this notebook has one additional feature extracted in feature extraction fucntion, which is spectral centroids.\n",
    "\n",
    "## total of 41 features are extracted, 40 mfcss and one spectral centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15625d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "datetime.datetime.now()\n",
    "import IPython.display as ipd\n",
    "num_of_mfccs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6c54c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>person-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>person-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>person-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>person-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>person-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>19_74</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>person-nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>19_75</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>person-nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>19_76</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>person-nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>19_77</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>person-nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>19_78</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>person-nineteen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1919 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     slice_file_name  fold  classID       class_name\n",
       "0                1_1     1        1       person-one\n",
       "1                1_2     1        1       person-one\n",
       "2                1_3     1        1       person-one\n",
       "3                1_4     1        1       person-one\n",
       "4                1_5     1        1       person-one\n",
       "...              ...   ...      ...              ...\n",
       "1914           19_74    19       19  person-nineteen\n",
       "1915           19_75    19       19  person-nineteen\n",
       "1916           19_76    19       19  person-nineteen\n",
       "1917           19_77    19       19  person-nineteen\n",
       "1918           19_78    19       19  person-nineteen\n",
       "\n",
       "[1919 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1919, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "metadata = pd.read_csv(r'custom_dataset\\metadata\\metadata.csv')\n",
    "display(metadata)\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6511621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type = 'kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc = num_of_mfccs)\n",
    "#         print(mfccs[0])\n",
    "#         one_mfcc  = mfccs[0]\n",
    "#         print(len(one_mfcc))\n",
    "#         mean = np.mean(one_mfcc)\n",
    "#         print(mean)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "    \n",
    "        spectral_centroids = librosa.feature.spectral_centroid(audio, sr=sample_rate)[0]\n",
    "        spectral_centroids = np.mean(spectral_centroids.T,axis=0)\n",
    "#         print(spectral_centroids)\n",
    "#         print(len(spectral_centroids))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None \n",
    "    \n",
    "    return mfccsscaled , spectral_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a332bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.2161481e+02  1.1762363e+02  1.1537064e+01  2.1561647e+01\n",
      "  1.1041671e+01  7.4994292e+00 -1.2753049e+01  4.1563697e+00\n",
      " -5.7044477e+00 -2.4767199e+00 -8.9030581e+00  4.4598341e+00\n",
      " -3.8210421e+00 -3.6995909e+00  1.5087281e+00 -3.3966599e+00\n",
      " -3.4508140e+00 -3.2805581e+00 -1.8277235e+00 -4.1847682e+00\n",
      " -1.1584170e+00 -1.6800704e+00 -3.0591936e+00 -7.0244282e-02\n",
      " -1.6802792e-01 -1.6682278e+00 -5.7726330e-01 -1.1854113e+00\n",
      " -9.1871601e-01 -2.9014547e+00 -1.7659615e+00 -1.8429492e+00\n",
      " -1.4652388e+00 -2.1726139e+00 -5.4019958e-01 -1.7346760e+00\n",
      " -1.0902280e+00 -1.9274250e+00 -1.6069030e+00 -1.7990167e+00]\n",
      "1686.360773772794\n"
     ]
    }
   ],
   "source": [
    "file = r'custom_dataset\\audio\\fold1\\1_1.wav'\n",
    "mfccsss , sc = extract_features(file)\n",
    "# print(mfccsss)\n",
    "print(mfccsss)\n",
    "print(sc)\n",
    "# print(mfccsscaleddd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7571fa",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfc2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  1919  files\n",
      "feature extraction completed in time:  0:03:10.302699\n"
     ]
    }
   ],
   "source": [
    "# data norm  = x - min / max - min\n",
    "import numpy as np\n",
    "# Set the path to the full dataset \n",
    "fulldatasetpath = r'custom_dataset\\audio'\n",
    "\n",
    "metadata = pd.read_csv(r'custom_dataset\\metadata\\metadata.csv')\n",
    "features = []\n",
    "start = datetime.datetime.now()\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'\\\\',str(row[\"slice_file_name\"])+\".wav\")\n",
    "    class_label = row[\"class_name\"]\n",
    "    data , s_centroids = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, s_centroids, class_label])\n",
    "    \n",
    "# Convert into a Panda dataframe\n",
    "featuresdf = pd.DataFrame(features, columns=['feature_mfcc', 'sc', 'class_label'])\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') \n",
    "\n",
    "duration = datetime.datetime.now() - start\n",
    "print(\"feature extraction completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a948c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_mfcc</th>\n",
       "      <th>sc</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-621.6148, 117.62363, 11.537064, 21.561647, 1...</td>\n",
       "      <td>1686.360774</td>\n",
       "      <td>Bilal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-572.908, 121.52725, 7.6588373, 11.633908, 3....</td>\n",
       "      <td>1509.653826</td>\n",
       "      <td>Bilal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-570.4361, 135.55675, -0.7183234, 14.39768, 3...</td>\n",
       "      <td>1486.397881</td>\n",
       "      <td>Bilal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-620.9414, 89.79995, 19.319904, 24.616987, 10...</td>\n",
       "      <td>1713.402361</td>\n",
       "      <td>Bilal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-626.43616, 121.38926, 14.657515, 17.612543, ...</td>\n",
       "      <td>1500.964248</td>\n",
       "      <td>Bilal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>[-335.83438, 148.57617, -49.6038, 44.24992, -1...</td>\n",
       "      <td>1530.514030</td>\n",
       "      <td>Babar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>[-358.63385, 127.80328, -21.487537, 39.676525,...</td>\n",
       "      <td>1602.068708</td>\n",
       "      <td>Babar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>[-369.32745, 150.0624, -38.85454, 22.66734, -2...</td>\n",
       "      <td>1509.565940</td>\n",
       "      <td>Babar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>[-343.89783, 141.7501, -38.138626, 31.375013, ...</td>\n",
       "      <td>1644.501594</td>\n",
       "      <td>Babar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>[-371.19647, 168.77455, -37.72717, 38.013287, ...</td>\n",
       "      <td>1254.551322</td>\n",
       "      <td>Babar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1919 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           feature_mfcc           sc  \\\n",
       "0     [-621.6148, 117.62363, 11.537064, 21.561647, 1...  1686.360774   \n",
       "1     [-572.908, 121.52725, 7.6588373, 11.633908, 3....  1509.653826   \n",
       "2     [-570.4361, 135.55675, -0.7183234, 14.39768, 3...  1486.397881   \n",
       "3     [-620.9414, 89.79995, 19.319904, 24.616987, 10...  1713.402361   \n",
       "4     [-626.43616, 121.38926, 14.657515, 17.612543, ...  1500.964248   \n",
       "...                                                 ...          ...   \n",
       "1914  [-335.83438, 148.57617, -49.6038, 44.24992, -1...  1530.514030   \n",
       "1915  [-358.63385, 127.80328, -21.487537, 39.676525,...  1602.068708   \n",
       "1916  [-369.32745, 150.0624, -38.85454, 22.66734, -2...  1509.565940   \n",
       "1917  [-343.89783, 141.7501, -38.138626, 31.375013, ...  1644.501594   \n",
       "1918  [-371.19647, 168.77455, -37.72717, 38.013287, ...  1254.551322   \n",
       "\n",
       "     class_label  \n",
       "0          Bilal  \n",
       "1          Bilal  \n",
       "2          Bilal  \n",
       "3          Bilal  \n",
       "4          Bilal  \n",
       "...          ...  \n",
       "1914       Babar  \n",
       "1915       Babar  \n",
       "1916       Babar  \n",
       "1917       Babar  \n",
       "1918       Babar  \n",
       "\n",
       "[1919 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(featuresdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8401ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "948272ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "# features are our independant variable\n",
    "X = np.array(featuresdf.feature_mfcc.tolist() , dtype=object)\n",
    "# print(type(X))\n",
    "X2 = np.array(featuresdf.sc.tolist() , dtype=object)\n",
    "\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "maximum = max(X2)\n",
    "minimum = min(X2)\n",
    "norm_X2 = []\n",
    "for i , val in enumerate(X2):\n",
    "    norm = ((val - minimum) / (maximum - minimum))\n",
    "    norm_X2.append(norm)\n",
    "    \n",
    "# print(type(X2))\n",
    "# print(X2)\n",
    "F = []\n",
    "for i , val in enumerate(X):\n",
    "    temp_x = val\n",
    "    temp_x2 = norm_X2[i]\n",
    "    \n",
    "#     concat = temp_x + temp_x2\n",
    "    concat = np.hstack((temp_x,temp_x2))\n",
    "    F.append(concat)\n",
    "    \n",
    "    \n",
    "# y dependant variable jo hum predict karenge.\n",
    "\n",
    "\n",
    "# print(len(X))\n",
    "print(len(X2))\n",
    "# print(len(y))\n",
    "\n",
    "# print(X[0])\n",
    "# print(X2[0])\n",
    "# print(y[0])\n",
    "\n",
    "# print(X[1])\n",
    "# print(X2[1])\n",
    "# print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c88dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2889.998644516788\n"
     ]
    }
   ],
   "source": [
    "print(max(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55df625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557.985370948461\n"
     ]
    }
   ],
   "source": [
    "print(min(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7182b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(max(norm_X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d2ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(min(norm_X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa4121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4838631990707972\n",
      "0.40808878162563816\n",
      "0.39811630615133026\n",
      "0.49545901104811296\n",
      "0.40436256850187374\n",
      "0.2250285780907718\n",
      "0.47485045875927673\n",
      "0.3983474900496715\n",
      "0.3891637449184772\n",
      "0.47510486098172483\n",
      "0.3651791268127439\n",
      "0.29196028022142684\n",
      "0.3566625051132655\n",
      "0.5627850874776309\n",
      "0.5627850874776309\n",
      "0.40271973048727494\n",
      "0.48174861974005173\n",
      "0.3670483941343302\n",
      "0.32900331656349047\n",
      "0.5302610354991779\n",
      "0.4550794982599829\n",
      "0.409780481848847\n",
      "0.4376515020595602\n",
      "0.47530159278163747\n",
      "0.5436139480738722\n",
      "0.4877676081418601\n",
      "0.4730993580107953\n",
      "0.5323350280402468\n",
      "0.5711107227558834\n",
      "0.5419299481484205\n",
      "0.5820394749437509\n",
      "0.6197738610207407\n",
      "0.7082417266375465\n",
      "0.631049228355886\n",
      "0.5038954263318814\n",
      "0.48999566235742015\n",
      "0.5382322040279378\n",
      "0.5271112622711956\n",
      "0.529540239904608\n",
      "0.6111021441026411\n",
      "0.5824925929276963\n",
      "0.5044342115417467\n",
      "0.5401369313415706\n",
      "0.4583159927953502\n",
      "0.5095956709738497\n",
      "0.5516587936970571\n",
      "0.49399915789278576\n",
      "0.5837712650918847\n",
      "0.5409098758833724\n",
      "0.5420636137567212\n",
      "0.6295676299334438\n",
      "0.34049135884386184\n",
      "0.301476524247857\n",
      "0.2586137260420695\n",
      "0.46714357208570245\n",
      "0.4462975808850034\n",
      "0.34599355077149546\n",
      "0.435193629936037\n",
      "0.4372623694304387\n",
      "0.5095911664757651\n",
      "0.5943698507344201\n",
      "0.45435148413963494\n",
      "0.3079127998241143\n",
      "0.4492972640798911\n",
      "0.3925143443720851\n",
      "0.33581258216471577\n",
      "0.4078204693506972\n",
      "0.44463402574472816\n",
      "0.45197203855896856\n",
      "0.40394526835766253\n",
      "0.4556935891020274\n",
      "0.426105873498467\n",
      "0.4566669326742097\n",
      "0.3912852657238375\n",
      "0.5143842805737976\n",
      "0.3007294378882376\n",
      "0.43301675263831474\n",
      "0.41806407953174507\n",
      "0.3133203129782433\n",
      "0.32110884297622233\n",
      "0.2631904215291729\n",
      "0.3318925704672653\n",
      "0.3687928023972858\n",
      "0.3873382010885541\n",
      "0.2692194007192641\n",
      "0.28627722528156474\n",
      "0.27967887864620167\n",
      "0.6084109439614439\n",
      "0.36517311588704643\n",
      "0.5823898155665792\n",
      "0.6344733021784142\n",
      "0.6264716719642301\n",
      "0.6588634816696162\n",
      "0.6272741996645866\n",
      "0.4528046368331364\n",
      "0.601985437895439\n",
      "0.6181279879424408\n",
      "0.6022682662939803\n",
      "0.4106399695903179\n",
      "0.4420655095737255\n",
      "0.30925277406942625\n",
      "0.4684086949855916\n",
      "0.36712730357158035\n",
      "0.21885254618599595\n",
      "0.561744890180298\n",
      "0.3017264482607893\n",
      "0.010808937093991933\n",
      "0.41394003404631\n",
      "0.3548079085118095\n",
      "0.32739233395372125\n",
      "0.27218127866495034\n",
      "0.36220489130772926\n",
      "0.2632991618196468\n",
      "0.28175238440004\n",
      "0.317430035839851\n",
      "0.2803036828784871\n",
      "0.24914045600828807\n",
      "0.37351126689445824\n",
      "0.41302320147343047\n",
      "0.18925700956339636\n",
      "0.3121028870273681\n",
      "0.342087390677303\n",
      "0.18275448432022529\n",
      "0.1994457919136202\n",
      "0.3191248724565429\n",
      "0.23127833467366282\n",
      "0.34305460733953635\n",
      "0.2464351214435562\n",
      "0.30913409749987725\n",
      "0.25908906070464066\n",
      "0.2847565837062356\n",
      "0.30321396198715406\n",
      "0.4387788327297865\n",
      "0.33635643192346043\n",
      "0.4454107468933615\n",
      "0.46721396289467615\n",
      "0.3241585723312707\n",
      "0.2834033077586809\n",
      "0.3199064181949271\n",
      "0.3451713785568039\n",
      "0.24647465530911386\n",
      "0.38664045269125824\n",
      "0.302804951078989\n",
      "0.17975006651072045\n",
      "0.22981782116261185\n",
      "0.3684652011486396\n",
      "0.427489382270001\n",
      "0.4486169713083802\n",
      "0.3197636005704671\n",
      "0.3763632822434401\n",
      "0.26568034108276783\n",
      "0.37304546451501724\n",
      "0.13099472907953175\n",
      "0.2254252034994549\n",
      "0.28540432711289865\n",
      "0.29730665682081253\n",
      "0.33834830286018214\n",
      "0.34964654531317\n",
      "0.23759268679275597\n",
      "0.375984713652302\n",
      "0.4758724312616053\n",
      "0.3389887363703104\n",
      "0.26013554277512174\n",
      "0.19669334501274893\n",
      "0.2866211119684969\n",
      "0.27383521540669037\n",
      "0.28072647466729395\n",
      "0.20734791913749323\n",
      "0.3022714360884986\n",
      "0.4092580998769037\n",
      "0.15050928913025516\n",
      "0.24094812928695789\n",
      "0.2989907758205764\n",
      "0.2165931035444998\n",
      "0.16596035616482982\n",
      "0.31192408728573107\n",
      "0.1608991643324553\n",
      "0.34196395733737167\n",
      "0.44127555218993053\n",
      "0.15432218176295212\n",
      "0.30294320081036336\n",
      "0.2800999256607867\n",
      "0.21758903624623321\n",
      "0.22069583089256747\n",
      "0.19543337596643054\n",
      "0.22050617017438054\n",
      "0.2053903423253221\n",
      "0.2422522539957947\n",
      "0.3718440783381275\n",
      "0.30147189945388786\n",
      "0.13513355617179976\n",
      "0.32950540453730465\n",
      "0.45160368599800443\n",
      "0.42175740019701113\n",
      "0.41221554380865555\n",
      "0.23923633771166689\n",
      "0.44774875141313664\n",
      "0.27948303810213937\n",
      "0.41493671096165263\n",
      "0.31544992603075195\n",
      "0.41394594803572193\n",
      "0.20680928643065147\n",
      "0.1109206175258362\n",
      "0.053337457672145865\n",
      "0.05056983873209477\n",
      "0.03046870813210991\n",
      "0.05942469372145219\n",
      "0.0\n",
      "0.11942275446741388\n",
      "0.07716274693729011\n",
      "0.04096661717054284\n",
      "0.058179450578290604\n",
      "0.052998041932026375\n",
      "0.038648023336895314\n",
      "0.03727071277270391\n",
      "0.09879296760293715\n",
      "0.10344846528716908\n",
      "0.08539426419719925\n",
      "0.22697268935027176\n",
      "0.3013389766902105\n",
      "0.07004437055606792\n",
      "0.10543211310290947\n",
      "0.14118510541601473\n",
      "0.13022106973919748\n",
      "0.14937477678772515\n",
      "0.12480664541831092\n",
      "0.16106143070532078\n",
      "0.17022554492156514\n",
      "0.14936924896019818\n",
      "0.21506442082270333\n",
      "0.124079794414702\n",
      "0.16333103025250095\n",
      "0.1594166993342126\n",
      "0.15644284301738\n",
      "0.23395120211092735\n",
      "0.1432041791904251\n",
      "0.1430825565310576\n",
      "0.1675588696821149\n",
      "0.1756942797677156\n",
      "0.15067255765228293\n",
      "0.1798763428179449\n",
      "0.12278072977864982\n",
      "0.12690807031363877\n",
      "0.17433976701122555\n",
      "0.14854376256602583\n",
      "0.1115475036636776\n",
      "0.12041572201126999\n",
      "0.07699460734742714\n",
      "0.12974793114836106\n",
      "0.13198982887051958\n",
      "0.1631519809420802\n",
      "0.13790248987145762\n",
      "0.18051819793983626\n",
      "0.04410789856794218\n",
      "0.07089441852873511\n",
      "0.07722624180624459\n",
      "0.14265165185294354\n",
      "0.18684051083531678\n",
      "0.11245967031089683\n",
      "0.13147467468970012\n",
      "0.1759773791381834\n",
      "0.2347087400626126\n",
      "0.16564296957303612\n",
      "0.1292697178915046\n",
      "0.07964029613155311\n",
      "0.13690736781282983\n",
      "0.09023001229323696\n",
      "0.1152868558663088\n",
      "0.09467549723126785\n",
      "0.15828282368900673\n",
      "0.06666841957834112\n",
      "0.08848284011137525\n",
      "0.043226208606431095\n",
      "0.1156448683115969\n",
      "0.12331123084142612\n",
      "0.09349500396905286\n",
      "0.14774938744623475\n",
      "0.08589060017030216\n",
      "0.15139879348569\n",
      "0.09812450539312109\n",
      "0.1602016822780716\n",
      "0.09146365228024883\n",
      "0.08657524458480126\n",
      "0.18043616287977934\n",
      "0.2004845467334009\n",
      "0.11882534203598263\n",
      "0.06533599352145651\n",
      "0.07160090174751008\n",
      "0.08629688239360421\n",
      "0.22408512887825055\n",
      "0.0987539363592238\n",
      "0.10071507146643617\n",
      "0.19107209860871946\n",
      "0.09956369612311768\n",
      "0.24524121340036248\n",
      "0.11722595090964895\n",
      "0.05783985102129123\n",
      "0.08078714003581468\n",
      "0.2523488276971914\n",
      "0.10861135784482163\n",
      "0.1731473068731027\n",
      "0.1703170640074964\n",
      "0.06462788424149446\n",
      "0.3306850646077027\n",
      "0.40208179344049727\n",
      "0.28385421144255063\n",
      "0.22581520127235144\n",
      "0.23997083971161776\n",
      "0.2087270977582709\n",
      "0.45826337190566463\n",
      "0.392478168309062\n",
      "0.31980729734230035\n",
      "0.4701628620001894\n",
      "0.4910123588257969\n",
      "0.3426398739209052\n",
      "0.25860087419418426\n",
      "0.6019439751639065\n",
      "0.4177449093469047\n",
      "0.4482573787069854\n",
      "0.4482573787069854\n",
      "0.2843916084745567\n",
      "0.2696474892284961\n",
      "0.33397441121574833\n",
      "0.308877056848397\n",
      "0.4120828368611043\n",
      "0.3742940448097511\n",
      "0.47189263907856294\n",
      "0.5682824713984125\n",
      "0.5010779353929065\n",
      "0.5010779353929065\n",
      "0.6349800564918422\n",
      "0.7800961804278306\n",
      "0.934770258991624\n",
      "0.7802970356969768\n",
      "0.5807248555029777\n",
      "0.6983589925328947\n",
      "0.5639468844033465\n",
      "0.5639468844033465\n",
      "0.5611795862295708\n",
      "0.5022205147763033\n",
      "0.5283981083375389\n",
      "0.39109390988117987\n",
      "0.5943591045431671\n",
      "0.6717761537448219\n",
      "0.6865356968671148\n",
      "0.5513697035816031\n",
      "0.6419506783419097\n",
      "0.5038324034940442\n",
      "0.5659357338852445\n",
      "0.49232653891382644\n",
      "0.6426708956372515\n",
      "0.6017467528104715\n",
      "0.4765512165361314\n",
      "0.5257577755140223\n",
      "0.3758392356664605\n",
      "0.3782337413901356\n",
      "0.34266917810964576\n",
      "0.5842829524325188\n",
      "0.6039160420509753\n",
      "0.34077067442467146\n",
      "0.3118312726715324\n",
      "0.34955630596009823\n",
      "0.4240866501659554\n",
      "0.4020133523775341\n",
      "0.531472580992098\n",
      "0.36480855486267666\n",
      "0.30787953111988775\n",
      "0.45198916002306716\n",
      "0.3464907102938007\n",
      "0.46944227465539634\n",
      "0.37491648351544377\n",
      "0.4214720337065013\n",
      "0.4347320790650933\n",
      "0.24023369921384705\n",
      "0.23962287024985307\n",
      "0.46522350734447804\n",
      "0.3569684399374558\n",
      "0.22057672291252634\n",
      "0.4163843380180468\n",
      "0.2555858046444734\n",
      "0.234503162665805\n",
      "0.2855850848628948\n",
      "0.3708356554315981\n",
      "0.5531080480101112\n",
      "0.3611161713709335\n",
      "0.20838214392634558\n",
      "0.3194525729663241\n",
      "0.37173995917354835\n",
      "0.3215336916459114\n",
      "0.22608308018301376\n",
      "0.2786201239827491\n",
      "0.19906000813336888\n",
      "0.4385570820851434\n",
      "0.37680978702904067\n",
      "0.3744151408809607\n",
      "0.5218101433024845\n",
      "0.3358555898776131\n",
      "0.24136026717744669\n",
      "0.3587668266499251\n",
      "0.4201658581763317\n",
      "0.3194459525063191\n",
      "0.299356129098687\n",
      "0.5380038957079757\n",
      "0.4055885612725457\n",
      "0.47824567960660275\n",
      "0.16217593704652822\n",
      "0.16217593704652822\n",
      "0.318517474343077\n",
      "0.5768935822260112\n",
      "0.34450389847643675\n",
      "0.5511700569426062\n",
      "0.4562495997244638\n",
      "0.4850684770339578\n",
      "0.3363855150982452\n",
      "0.3918058346725006\n",
      "0.341578011940922\n",
      "0.27852037737872254\n",
      "0.4918563467916445\n",
      "0.21434991841556095\n",
      "0.3759701314309856\n",
      "0.4610800703876124\n",
      "0.263146440789043\n",
      "0.3110574780452821\n",
      "0.4262899186786871\n",
      "0.4113970026520513\n",
      "0.4172498106047793\n",
      "0.46366401252361145\n",
      "0.49110522665880013\n",
      "0.40640592848349866\n",
      "0.3688110160974621\n",
      "0.40640592848349866\n",
      "0.3786761191581041\n",
      "0.5638145390190129\n",
      "0.4584860290376442\n",
      "0.3524388483827607\n",
      "0.2975986238778645\n",
      "0.4977101122020077\n",
      "0.7443135645880121\n",
      "0.7068763009543632\n",
      "0.7794263443031922\n",
      "0.6432260351881176\n",
      "0.697288659396208\n",
      "0.8191154140639668\n",
      "0.7443135645880121\n",
      "0.7443135645880121\n",
      "0.5579942968646043\n",
      "0.7443135645880121\n",
      "0.5579942968646043\n",
      "0.6600146976368289\n",
      "0.5579942968646043\n",
      "0.48137686086080567\n",
      "0.6167738644650949\n",
      "0.6868275369824901\n",
      "0.6009312310045033\n",
      "0.6167738644650949\n",
      "0.4895055335664582\n",
      "0.657400522095347\n",
      "0.4709805639230852\n",
      "0.5143450551789889\n",
      "0.5873599004425919\n",
      "0.35195979614733497\n",
      "0.3134030141956372\n",
      "0.5731915120401542\n",
      "0.5099305498769977\n",
      "0.42749822150924205\n",
      "0.2667137458151357\n",
      "0.22920176223599992\n",
      "0.3047094766482901\n",
      "0.7349299074148089\n",
      "0.4036821609291761\n",
      "0.39639657949961127\n",
      "0.5745968904747779\n",
      "0.34917131239534194\n",
      "0.2588961821523091\n",
      "0.20140534973087698\n",
      "0.47309256335292654\n",
      "0.38613190245910345\n",
      "0.22706526024444673\n",
      "0.3580091762999422\n",
      "0.1494261183201577\n",
      "0.4730979535954121\n",
      "0.5512721955826121\n",
      "0.5338762065557389\n",
      "0.3928991036536204\n",
      "0.3990337703352944\n",
      "0.3688625991525413\n",
      "0.277268227426934\n",
      "0.4422641662591918\n",
      "0.36519654266358337\n",
      "0.20859495388644145\n",
      "0.30881536726309095\n",
      "0.4855195938930583\n",
      "0.6084291049142749\n",
      "0.060240230536791124\n",
      "0.5257076362468014\n",
      "0.43700167800648243\n",
      "0.46746155631355807\n",
      "0.3669589714884835\n",
      "0.22091683610774762\n",
      "0.42071363691779234\n",
      "0.22694842349390587\n",
      "0.3461241522867185\n",
      "0.2383212330285775\n",
      "0.4342525123046468\n",
      "0.34342344136489894\n",
      "0.6579683966793971\n",
      "0.5007344121585079\n",
      "0.4245849307296165\n",
      "0.5510067672506636\n",
      "0.6771740758990814\n",
      "0.2272271115687112\n",
      "0.5828804644241261\n",
      "0.50444101325479\n",
      "0.43886897416411774\n",
      "0.44422898423327317\n",
      "0.5680811765092653\n",
      "0.5197470652184779\n",
      "0.31209416682198887\n",
      "0.49792334086656653\n",
      "0.6648601210255791\n",
      "0.4570489904600724\n",
      "0.7004509427886808\n",
      "0.5053613204750076\n",
      "0.36900217242545985\n",
      "0.4239314261110315\n",
      "0.5305761175512665\n",
      "0.47295497273277176\n",
      "0.47576978705371553\n",
      "0.5934245485456778\n",
      "0.5161805205768212\n",
      "0.5726667288104615\n",
      "0.5726667288104615\n",
      "0.6345909313127492\n",
      "0.6556349381713172\n",
      "0.6895933310476133\n",
      "0.6207919152689882\n",
      "0.8001116086518885\n",
      "0.7340342430088393\n",
      "0.7794243049354229\n",
      "0.7593872672855738\n",
      "0.7219991155687786\n",
      "0.7241004036326824\n",
      "0.7191592682741249\n",
      "0.7106352161730493\n",
      "0.7694855205443925\n",
      "0.769467983776216\n",
      "0.7994883178624527\n",
      "0.7649401308983459\n",
      "0.6802196305924032\n",
      "0.7161378206709551\n",
      "0.711174894437208\n",
      "0.7133985600656593\n",
      "0.7960459225397851\n",
      "0.7391218380658048\n",
      "0.7547144514827907\n",
      "0.7406644776379773\n",
      "0.5871543746178132\n",
      "0.42874406873975374\n",
      "0.5165907574717683\n",
      "0.6345748920231049\n",
      "0.6908227584608199\n",
      "0.5724092172454519\n",
      "0.5640112745957161\n",
      "0.6354898812682769\n",
      "0.740342431888554\n",
      "0.6893845144663298\n",
      "0.45090229855617586\n",
      "0.37581191006520237\n",
      "0.5315433536882824\n",
      "0.4783916346405397\n",
      "0.5114493916024182\n",
      "0.39401276822071746\n",
      "0.4535376638055053\n",
      "0.47149412423438575\n",
      "0.32912186380338543\n",
      "0.4077352780267413\n",
      "0.4820835081904201\n",
      "0.6231377444516727\n",
      "0.4365752942565321\n",
      "0.5730694885784582\n",
      "0.30464310571451897\n",
      "0.4623690314379347\n",
      "0.5297823784292081\n",
      "0.3551626872293837\n",
      "0.440106639668989\n",
      "0.4103039486127372\n",
      "0.5284776562395547\n",
      "0.5205712972969824\n",
      "0.4690504840252241\n",
      "0.35387559617416553\n",
      "0.3331490326409725\n",
      "0.36432384596021034\n",
      "0.6326895960595099\n",
      "0.3909267124158033\n",
      "0.38480337243469004\n",
      "0.5830886960070822\n",
      "0.4781042170137624\n",
      "0.5975391382099177\n",
      "0.5999762768394782\n",
      "0.509870593580292\n",
      "0.62560832809184\n",
      "0.4458639062269746\n",
      "0.6158118122590562\n",
      "0.4693527063893782\n",
      "0.5798875964451288\n",
      "0.38408598785381176\n",
      "0.5000080610770106\n",
      "0.5815854232448104\n",
      "0.4983464508602967\n",
      "0.553378269342197\n",
      "0.5538492661064783\n",
      "0.19990689417945798\n",
      "0.5415114450989501\n",
      "0.45010749726050675\n",
      "0.5298331184665496\n",
      "0.43732141493182514\n",
      "0.5640522456539978\n",
      "0.4538352900668217\n",
      "0.48421271583985914\n",
      "0.5403011342820497\n",
      "0.6951756934228541\n",
      "0.5052525289009546\n",
      "0.7822690006536899\n",
      "0.5801046789401341\n",
      "0.5100539407564506\n",
      "0.7341175013068343\n",
      "0.5803718840156853\n",
      "0.520102797189449\n",
      "0.5785779968354964\n",
      "0.7118603804954733\n",
      "0.6685702248797988\n",
      "0.7019502850971533\n",
      "0.6820998183756131\n",
      "0.6863827361522078\n",
      "0.7268716028180807\n",
      "0.6989336994027112\n",
      "0.6650937552147566\n",
      "0.6655128394027422\n",
      "0.7348266725459995\n",
      "0.792662344292656\n",
      "0.792662344292656\n",
      "0.8347689438201349\n",
      "0.8811003241666949\n",
      "0.8443558633395538\n",
      "0.875554277086882\n",
      "0.8287631158213532\n",
      "0.8589123629548787\n",
      "0.9217183389800251\n",
      "0.9701866926799898\n",
      "0.6230771006890627\n",
      "0.8031656093865506\n",
      "0.7871804383855582\n",
      "0.6629369029538729\n",
      "0.8228428147704987\n",
      "0.8389105332746417\n",
      "0.7736163951608612\n",
      "0.7996317139431823\n",
      "0.6822746045381449\n",
      "0.5700818590568705\n",
      "0.6610898009900299\n",
      "0.7838071876796401\n",
      "0.7102659057947122\n",
      "0.6231747630313463\n",
      "0.7004359714885078\n",
      "0.6747715094444495\n",
      "0.7428355034324918\n",
      "0.7550812396661327\n",
      "0.5902939776809432\n",
      "0.540896596237127\n",
      "0.585763983450171\n",
      "0.5619248653724488\n",
      "0.6186226597939297\n",
      "0.6013733102458777\n",
      "0.6108833754432942\n",
      "0.6740637287886483\n",
      "0.5857996704428008\n",
      "0.6163395328028799\n",
      "0.63549971472841\n",
      "0.6836605201475823\n",
      "0.5038120353411439\n",
      "0.7187243855940582\n",
      "0.5385449598448945\n",
      "0.5943638051406739\n",
      "0.6279758264649682\n",
      "0.5807798751886559\n",
      "0.588319526619822\n",
      "0.5734784814913024\n",
      "0.6470405256588114\n",
      "0.7102516714165606\n",
      "0.6453870873563377\n",
      "0.5788937785614662\n",
      "0.5646287991933401\n",
      "0.5438456114491519\n",
      "0.8219623298325353\n",
      "0.5956221055847848\n",
      "0.5502625720273312\n",
      "0.6465389133516164\n",
      "0.5543737089698793\n",
      "0.7538571702924112\n",
      "0.824368493939303\n",
      "0.4993651295111911\n",
      "0.7748075605859305\n",
      "0.6606336275019958\n",
      "0.6691271657036758\n",
      "0.6825132196556957\n",
      "0.5274490418248875\n",
      "0.2912267514386452\n",
      "0.43563977397141884\n",
      "0.42387166825603667\n",
      "0.46735126439516356\n",
      "0.49333802936748966\n",
      "0.5179596547127258\n",
      "0.19385065453008704\n",
      "0.6120796318039622\n",
      "0.4360308427139182\n",
      "0.45501815015449404\n",
      "0.39982142089937833\n",
      "0.49221218076714846\n",
      "0.4508116210772733\n",
      "0.31301922691853346\n",
      "0.4412978424074561\n",
      "0.5117371404489099\n",
      "0.36851660409388715\n",
      "0.5430732697492955\n",
      "0.35178566672759887\n",
      "0.41086264065232214\n",
      "0.5208628980929206\n",
      "0.420344746839554\n",
      "0.4357236024951565\n",
      "0.44613135028904916\n",
      "0.5539356936271282\n",
      "0.5052814880528819\n",
      "0.47477575248199566\n",
      "0.5214639910098106\n",
      "0.4963950955945099\n",
      "0.479626460777941\n",
      "0.49477951624012\n",
      "0.5034598906377061\n",
      "0.7131891568973198\n",
      "0.6871230329023081\n",
      "0.7739523001343583\n",
      "0.8037256464422362\n",
      "0.6726534253662668\n",
      "0.5929192222323723\n",
      "0.6302304735148514\n",
      "0.6056836716577151\n",
      "0.6928381203347093\n",
      "0.6941694431458652\n",
      "0.7524473774800245\n",
      "0.6806481728922281\n",
      "0.5988924265640171\n",
      "0.5958326387401726\n",
      "0.6296865113579759\n",
      "0.735772626991496\n",
      "0.7691928457303828\n",
      "0.7720347014622955\n",
      "0.690861891106521\n",
      "0.6865857646610781\n",
      "0.4261322209100974\n",
      "0.3148933409371775\n",
      "0.3607769875738174\n",
      "0.6155553478625119\n",
      "0.49223206136142833\n",
      "0.4426477076463394\n",
      "0.5019314155974933\n",
      "0.5763789538284347\n",
      "0.5807529454388348\n",
      "0.606147095588884\n",
      "0.4139109468503733\n",
      "0.3603541145812234\n",
      "0.44643452475138096\n",
      "0.5056138891508832\n",
      "0.48814759878941133\n",
      "0.46225109440843126\n",
      "0.37619458794333166\n",
      "0.4235944596582424\n",
      "0.3736433768703465\n",
      "0.47406574418126923\n",
      "0.545716489167129\n",
      "0.5235847966399771\n",
      "0.4102973483154008\n",
      "0.55373531793989\n",
      "0.39007980173142537\n",
      "0.5684174528107953\n",
      "0.6407228864931573\n",
      "0.43132968816929457\n",
      "0.4325942806286519\n",
      "0.3882453892144927\n",
      "0.5183173937393315\n",
      "0.49508433508721217\n",
      "0.46211521588476656\n",
      "0.4059959543390674\n",
      "0.3626911541782047\n",
      "0.3882989625626531\n",
      "0.7037135528238412\n",
      "0.37157264941484497\n",
      "0.4470636141659136\n",
      "0.5499857123213509\n",
      "0.5395619831449502\n",
      "0.7186806382241325\n",
      "0.5621535166869447\n",
      "0.5068455428833553\n",
      "0.6680814882880314\n",
      "0.5699810831497362\n",
      "0.6631116552973422\n",
      "0.5465208248979342\n",
      "0.6153867552062914\n",
      "0.4422456324822922\n",
      "0.3263609040275948\n",
      "0.25729805064530964\n",
      "0.2623799536754986\n",
      "0.2780336487126418\n",
      "0.37051109241967484\n",
      "0.1387190316368335\n",
      "0.3833970096105229\n",
      "0.3307891524062756\n",
      "0.3008283192655185\n",
      "0.4078537645755464\n",
      "0.3672906351412915\n",
      "0.32386684789749753\n",
      "0.2222554326837082\n",
      "0.33380644763788153\n",
      "0.4678980662123705\n",
      "0.29169442270495016\n",
      "0.5173782098803591\n",
      "0.29739534141424195\n",
      "0.30856224471138727\n",
      "0.3880364726825301\n",
      "0.3804928948019158\n",
      "0.3861607186753792\n",
      "0.3934631229244021\n",
      "0.4969766463977288\n",
      "0.44060579566920577\n",
      "0.45376604237296314\n",
      "0.43624582272867474\n",
      "0.42485786422617516\n",
      "0.4163521115975932\n",
      "0.4614211414958045\n",
      "0.42962429139362635\n",
      "0.546892986159377\n",
      "0.647295705143685\n",
      "0.5691638817775866\n",
      "0.5791441133712504\n",
      "0.4106641914073824\n",
      "0.5039061547926663\n",
      "0.4858210789448114\n",
      "0.4493904420902826\n",
      "0.5728025364886906\n",
      "0.5046523763308743\n",
      "0.6053604904124272\n",
      "0.5566178546193883\n",
      "0.410957312147257\n",
      "0.48751214237344176\n",
      "0.4098232367062796\n",
      "0.47812495758688134\n",
      "0.5389145079808727\n",
      "0.5020327811229643\n",
      "0.5089807626404751\n",
      "0.4839022364932295\n",
      "0.30220891929674326\n",
      "0.25774809485505296\n",
      "0.24766207432562484\n",
      "0.3588325307490307\n",
      "0.37305604381320845\n",
      "0.3456589094573422\n",
      "0.2926704570748739\n",
      "0.35424288199513504\n",
      "0.35534831070006273\n",
      "0.3776886295440939\n",
      "0.2391960229850844\n",
      "0.24599280028376863\n",
      "0.2999256724731513\n",
      "0.32644366281030907\n",
      "0.34026582607407146\n",
      "0.27766889306227455\n",
      "0.21424972732694952\n",
      "0.2769679964225806\n",
      "0.21971839400898407\n",
      "0.34414223598948634\n",
      "0.3855028507052541\n",
      "0.31622504358955517\n",
      "0.30640017150492055\n",
      "0.37561137601575223\n",
      "0.28501572974175815\n",
      "0.3386430802094944\n",
      "0.3965398158703737\n",
      "0.2519651352319066\n",
      "0.32979330780153093\n",
      "0.2639226617984757\n",
      "0.35765780450292833\n",
      "0.3834357194669676\n",
      "0.33925928999248567\n",
      "0.2858286539881355\n",
      "0.2980977108530775\n",
      "0.21807475001889265\n",
      "0.45627924253065066\n",
      "0.2679824146383956\n",
      "0.2741470336607083\n",
      "0.39196962323361884\n",
      "0.3441897765374792\n",
      "0.42295510868739494\n",
      "0.47671365709706104\n",
      "0.324111487525371\n",
      "0.38361740890295\n",
      "0.26862678906473597\n",
      "0.3832676698258106\n",
      "0.36766015026000903\n",
      "0.37903405381810434\n",
      "0.25081331715529803\n",
      "0.37101772942739975\n",
      "0.21388081048804478\n",
      "0.3592427137138619\n",
      "0.3254985748088273\n",
      "0.4514863521242789\n",
      "0.0339040514873275\n",
      "0.4794536192151513\n",
      "0.22477977477304786\n",
      "0.25431311079942714\n",
      "0.3114155476593085\n",
      "0.3238011134239931\n",
      "0.12394673122562846\n",
      "0.1540171358491486\n",
      "0.35431888785444426\n",
      "0.43125589925406993\n",
      "0.17764292477350754\n",
      "0.5222803536314203\n",
      "0.28842631820519066\n",
      "0.2621308439776325\n",
      "0.3026719230679218\n",
      "0.4709287354042113\n",
      "0.4419051555828906\n",
      "0.4087882701369273\n",
      "0.47045917268769855\n",
      "0.4557875675070034\n",
      "0.4859259510726014\n",
      "0.5103137906320451\n",
      "0.43769273980265594\n",
      "0.4139852113148048\n",
      "0.4719345399114778\n",
      "0.41487769286892884\n",
      "0.5808666709925645\n",
      "0.5136853089900313\n",
      "0.4968782801220888\n",
      "0.5808666709925645\n",
      "0.5955037586460364\n",
      "0.4406662843594768\n",
      "0.6025386899224793\n",
      "0.5955037586460364\n",
      "0.5880333782062876\n",
      "0.6108289453126715\n",
      "0.5919507207570063\n",
      "0.7074492943567501\n",
      "0.42660775861422195\n",
      "0.5032110056600374\n",
      "0.47420940047835225\n",
      "0.4657947963964386\n",
      "0.5157381804508964\n",
      "0.48711247863723806\n",
      "0.4503271407391061\n",
      "0.6293680483030277\n",
      "0.31807946561169387\n",
      "0.26951490707528225\n",
      "0.3013184161284714\n",
      "0.45290546521506314\n",
      "0.49149354487764596\n",
      "0.33187138102558866\n",
      "0.41204057554714174\n",
      "0.4816513580671513\n",
      "0.4911225770236462\n",
      "0.47346309859009716\n",
      "0.272537017335486\n",
      "0.2833380445570108\n",
      "0.3488274502156037\n",
      "0.3329651971574888\n",
      "0.4070507469291937\n",
      "0.37175423710866917\n",
      "0.3387252462276454\n",
      "0.40666909465396284\n",
      "0.3592305943790625\n",
      "0.3205099702944734\n",
      "0.38844019765235605\n",
      "0.4088510641519478\n",
      "0.2653172690983993\n",
      "0.44774145526621834\n",
      "0.2961785509284665\n",
      "0.29622406409989643\n",
      "0.3989757830967019\n",
      "0.38249808332479324\n",
      "0.30644733416374165\n",
      "0.2925171400090707\n",
      "0.42727229655323473\n",
      "0.38704561394773196\n",
      "0.3599392326756581\n",
      "0.2556093685664213\n",
      "0.2875567789413616\n",
      "0.2759133469951611\n",
      "0.5588199840239313\n",
      "0.28673105224418466\n",
      "0.25971738472393063\n",
      "0.4169589721491448\n",
      "0.43886002140626823\n",
      "0.49359071258266607\n",
      "0.5212362830022873\n",
      "0.27069672242662984\n",
      "0.4072396463506506\n",
      "0.33956688771079346\n",
      "0.47024164501527904\n",
      "0.3037405431508103\n",
      "0.44614359074534987\n",
      "0.29176206400636556\n",
      "0.38942672164407194\n",
      "0.38795343138039\n",
      "0.2970182499055208\n",
      "0.44066864129025457\n",
      "0.3711074526877821\n",
      "0.1360554564285181\n",
      "0.40596771140263943\n",
      "0.25534455088815006\n",
      "0.26145688514598375\n",
      "0.24593738152563144\n",
      "0.34728643783434304\n",
      "0.4168852174975665\n",
      "0.2637019893428005\n",
      "0.32891866426514715\n",
      "0.5445597157511535\n",
      "0.30332984903556887\n",
      "0.4618673315171666\n",
      "0.38398571816356397\n",
      "0.2661750083167477\n",
      "0.3422474596450382\n",
      "0.5136925743121563\n",
      "0.44416989675031754\n",
      "0.4253228603192065\n",
      "0.4251438095499986\n",
      "0.39305302112421275\n",
      "0.41647747607812624\n",
      "0.42408061883484655\n",
      "0.36779498256631793\n",
      "0.41577467942114604\n",
      "0.4960396173576644\n",
      "0.4416665645268788\n",
      "0.5248114610006882\n",
      "0.4191758500558654\n",
      "0.41990684052508037\n",
      "0.5284593279414411\n",
      "0.44881606659156104\n",
      "0.4868167217682409\n",
      "0.4138807333295899\n",
      "0.4676869077341774\n",
      "0.5021624523979039\n",
      "0.5581425808085901\n",
      "0.5519463067038396\n",
      "0.4860025417023457\n",
      "0.39965696215257274\n",
      "0.36871482136677225\n",
      "0.48038009968195244\n",
      "0.4465078558160715\n",
      "0.5445548515310339\n",
      "0.4461565234445614\n",
      "0.5004696256235817\n",
      "0.46173179213756865\n",
      "0.2688779746086364\n",
      "0.29703074557140874\n",
      "0.34628764303987286\n",
      "0.3907084698086393\n",
      "0.45999018820849563\n",
      "0.3637112940691611\n",
      "0.3521269206141059\n",
      "0.44146005166111235\n",
      "0.36353574980461345\n",
      "0.5384530160261142\n",
      "0.38551785200739686\n",
      "0.3623074258125412\n",
      "0.49584645608767763\n",
      "0.28300624325003504\n",
      "0.44957684955653804\n",
      "0.4026242168430496\n",
      "0.38783912377780194\n",
      "0.40483108330410716\n",
      "0.42071506801307407\n",
      "0.24474855811188892\n",
      "0.3400886463439969\n",
      "0.3346895867811867\n",
      "0.279730679421383\n",
      "0.3629619162533933\n",
      "0.37255820385973054\n",
      "0.37197513065832993\n",
      "0.3867455771071736\n",
      "0.274871202163062\n",
      "0.4000605215586837\n",
      "0.25930774770535864\n",
      "0.5514472625353138\n",
      "0.43888467501491457\n",
      "0.450829873457568\n",
      "0.38138978408656754\n",
      "0.3503217807332049\n",
      "0.25794712267203884\n",
      "0.6012274344202369\n",
      "0.3383789583672515\n",
      "0.28616926082551414\n",
      "0.41918333413453984\n",
      "0.3907062697524402\n",
      "0.3675858576555316\n",
      "0.5390542600404825\n",
      "0.3500232131003871\n",
      "0.5163882847665734\n",
      "0.2965757768682365\n",
      "0.45728963521748767\n",
      "0.31448336197804533\n",
      "0.3897347762451653\n",
      "0.2859995897880122\n",
      "0.510005893379865\n",
      "0.6576193204587764\n",
      "0.4855438618463129\n",
      "0.7607069646992409\n",
      "0.6866999708225083\n",
      "0.17292725553434188\n",
      "0.7395902843333354\n",
      "0.4216654411827951\n",
      "0.6060726831814621\n",
      "0.5358931760045123\n",
      "0.46543411574049853\n",
      "0.44180221279955667\n",
      "0.3612503434952993\n",
      "0.4456979909158062\n",
      "0.5483903316010944\n",
      "0.3971799550053961\n",
      "0.8242963589291012\n",
      "0.5463250954062354\n",
      "0.4371323492607207\n",
      "0.44739252891363174\n",
      "0.5302913471793343\n",
      "0.6598431250373238\n",
      "0.5824907063486203\n",
      "0.6392758141802533\n",
      "0.5980103905056575\n",
      "0.6017147113394405\n",
      "0.6392758141802533\n",
      "0.7005843025464656\n",
      "0.6581242418098587\n",
      "0.6293857143992622\n",
      "0.7005843025464656\n",
      "0.7149486071396439\n",
      "0.7753559027008958\n",
      "0.7188552424086438\n",
      "0.7149486071396439\n",
      "0.635663027672242\n",
      "0.5608426015332825\n",
      "0.7034774314747348\n",
      "0.635663027672242\n",
      "0.7841887678723609\n",
      "0.8350685725314084\n",
      "0.6841550601980286\n",
      "0.7841887678723609\n",
      "0.5830096874449153\n",
      "0.6137759688913808\n",
      "0.560696008944299\n",
      "0.5830096874449153\n",
      "0.709729924050898\n",
      "0.6092932001542557\n",
      "0.6864662664734427\n",
      "0.709729924050898\n",
      "0.3705249556034974\n",
      "0.40950066003977975\n",
      "0.49627743371594324\n",
      "0.6461117123336492\n",
      "0.6144702271334345\n",
      "0.44885692103550745\n",
      "0.3531878465578955\n",
      "0.45484312658760007\n",
      "0.535033106993792\n",
      "0.5872135631437606\n",
      "0.5714316835716516\n",
      "0.3344179988328096\n",
      "0.4042330905260039\n",
      "0.29997806303881575\n",
      "0.44029407659305975\n",
      "0.3386905797918774\n",
      "0.3789794358761969\n",
      "0.4103739305681755\n",
      "0.337208129928357\n",
      "0.3411616874142723\n",
      "0.4085717944228101\n",
      "0.45469195396849826\n",
      "0.2831342391701068\n",
      "0.46176459231753997\n",
      "0.2635449252548316\n",
      "0.3971851784588967\n",
      "0.4120772160470241\n",
      "0.32197553850026317\n",
      "0.4127245679949553\n",
      "0.29032692532943244\n",
      "0.5024047234018838\n",
      "0.5026128024986356\n",
      "0.4054319959489713\n",
      "0.28092135623314857\n",
      "0.31686445953665643\n",
      "0.2564316224527711\n",
      "0.5339964760368641\n",
      "0.3607708728792659\n",
      "0.2779713527173751\n",
      "0.3422243011876419\n",
      "0.38650541810292116\n",
      "0.4418494504312781\n",
      "0.5056413560724322\n",
      "0.3286956709833983\n",
      "0.41211406403580964\n",
      "0.4167814368625802\n",
      "0.508304092808952\n",
      "0.3158797556574027\n",
      "0.49440764787941044\n",
      "0.30656608748669384\n",
      "0.6997130157946755\n",
      "0.6258787625335169\n",
      "0.45907177520773945\n",
      "0.5620261694454547\n",
      "0.5842391285470316\n",
      "0.44420876026754436\n",
      "0.6333195909605408\n",
      "0.6711434541057623\n",
      "0.6094082885377852\n",
      "0.43112440556943943\n",
      "0.39933827759960877\n",
      "0.35111962304537375\n",
      "0.31324596203752375\n",
      "0.7003636319690985\n",
      "0.46034700421733343\n",
      "0.5568272918816732\n",
      "0.5154062345104683\n",
      "0.46017684816485444\n",
      "0.607727363583835\n",
      "0.7333125482930963\n",
      "0.7717644723986503\n",
      "0.7838352220470943\n",
      "0.7316672162898888\n",
      "0.7959656096838095\n",
      "0.7413961635084089\n",
      "0.7169347341449126\n",
      "0.7959656096838095\n",
      "0.8097264870941379\n",
      "0.8081976894190772\n",
      "0.7899656722564343\n",
      "0.8097264870941379\n",
      "0.8970254242399691\n",
      "0.8669419350340165\n",
      "1.0\n",
      "0.8970254242399691\n",
      "0.8774244767388272\n",
      "0.7827548215892584\n",
      "0.7884486212057611\n",
      "0.8774244767388272\n",
      "0.8234828519904441\n",
      "0.9255814382195328\n",
      "0.8822328755698796\n",
      "0.8234828519904441\n",
      "0.5543666088557784\n",
      "0.7085915750661714\n",
      "0.790116786003117\n",
      "0.5543666088557784\n",
      "0.8982360467351767\n",
      "0.6833033391138785\n",
      "0.7360803062283956\n",
      "0.8982360467351767\n",
      "0.6037145084090002\n",
      "0.5066481868392522\n",
      "0.6655862545231903\n",
      "0.7690614623961792\n",
      "0.7074247307530603\n",
      "0.5751560938616577\n",
      "0.5640182562330343\n",
      "0.6464691207458862\n",
      "0.7654083064076108\n",
      "0.6555465411351044\n",
      "0.6500498407144478\n",
      "0.6371074366032059\n",
      "0.5003042210444325\n",
      "0.49984729242963427\n",
      "0.5612838835630477\n",
      "0.6008112169604348\n",
      "0.38004050611405515\n",
      "0.5624705572200196\n",
      "0.6059696061238411\n",
      "0.42443569173474327\n",
      "0.4299257521407874\n",
      "0.5132854994253268\n",
      "0.6314721903768145\n",
      "0.43581661569141\n",
      "0.43060783270261943\n",
      "0.48736680859863935\n",
      "0.5566521746366566\n",
      "0.4399758238613284\n",
      "0.46504117501868564\n",
      "0.44532680649047546\n",
      "0.575148880161968\n",
      "0.5848952542034151\n",
      "0.5604151096807182\n",
      "0.44429686155024745\n",
      "0.3717146520604387\n",
      "0.3344845167497913\n",
      "0.4738929777556853\n",
      "0.39888783240335285\n",
      "0.5473034759312734\n",
      "0.549748798055258\n",
      "0.4577777990533381\n",
      "0.5220401563806546\n",
      "0.4531083284599271\n",
      "0.45130955650643384\n",
      "0.5306761635379645\n",
      "0.5347784180371234\n",
      "0.5347784180371234\n",
      "0.45869247047122474\n",
      "0.5038632703716266\n",
      "0.40625740180572767\n",
      "0.44979772695950626\n",
      "0.1622041870278598\n",
      "0.14858880637186442\n",
      "0.37951247061906146\n",
      "0.3756485846715491\n",
      "0.1546274357989905\n",
      "0.4368406010387884\n",
      "0.4368406010387884\n",
      "0.3288597530994157\n",
      "0.3559058411857536\n",
      "0.40207223673757003\n",
      "0.3577998834429143\n",
      "0.28245147410197385\n",
      "0.36444089901392845\n",
      "0.28555442237676226\n",
      "0.6099865485766703\n",
      "0.44979010957727855\n",
      "0.7962882101945035\n",
      "0.4253987124737476\n",
      "0.31166688382879876\n",
      "0.42474589596967693\n",
      "0.44148404698016175\n",
      "0.5615618882566143\n",
      "0.5830372737534399\n",
      "0.5393293726382501\n",
      "0.47171471997570513\n",
      "0.5830372737534399\n",
      "0.4910271135952966\n",
      "0.5079476461175666\n",
      "0.4757042147919697\n",
      "0.4910271135952966\n",
      "0.5076017144075895\n",
      "0.5538545736698108\n",
      "0.4894886823860655\n",
      "0.5076017144075895\n",
      "0.5382015374510756\n",
      "0.46695203913451\n",
      "0.5377948215269883\n",
      "0.5382015374510756\n",
      "0.5831254883440127\n",
      "0.4573946395127741\n",
      "0.6409435673438019\n",
      "0.4573946395127741\n",
      "0.4107479020743992\n",
      "0.39576191452447895\n",
      "0.48566141761307063\n",
      "0.39576191452447895\n",
      "0.40130678769048306\n",
      "0.48798135959699235\n",
      "0.4902307234659827\n",
      "0.48798135959699235\n",
      "0.37684641284355724\n",
      "0.32916482716236356\n",
      "0.21875451634052234\n",
      "0.3785661121673897\n",
      "0.45577592941391115\n",
      "0.4395553368632785\n",
      "0.2875809590413916\n",
      "0.3987927270964081\n",
      "0.48724438310389784\n",
      "0.3978447769488625\n",
      "0.4779879888401155\n",
      "0.31651962328634903\n",
      "0.2031322947002773\n",
      "0.3690724117953531\n",
      "0.23371376867894264\n",
      "0.34224388775282466\n",
      "0.28616513280905354\n",
      "0.345519758826745\n",
      "0.3814580651258303\n",
      "0.22266812219993604\n",
      "0.31305401606273087\n",
      "0.2626474581830284\n",
      "0.35014886472111034\n",
      "0.21609940627910412\n",
      "0.3652344489590838\n",
      "0.2693343385429087\n",
      "0.29782011213249704\n",
      "0.3004465725017507\n",
      "0.32662606568960084\n",
      "0.3790420765084874\n",
      "0.29280697557555796\n",
      "0.45084050985596696\n",
      "0.34660811503138683\n",
      "0.30693576459125943\n",
      "0.24423914999796725\n",
      "0.1942961543851747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4553092899067009\n",
      "0.24070570740574157\n",
      "0.19509906773385802\n",
      "0.4559899318451163\n",
      "0.3107578019462672\n",
      "0.5959552924433049\n",
      "0.4856741095872886\n",
      "0.35473514705558895\n",
      "0.374814504349954\n",
      "0.42275807249240505\n",
      "0.282721263400281\n",
      "0.37466560667693377\n",
      "0.25533913291590793\n",
      "0.2852627090935515\n",
      "0.49773709949775086\n",
      "0.5482950076765264\n",
      "0.5088922079762891\n",
      "0.6332200532247197\n",
      "0.5555540640326494\n",
      "0.22640600446201858\n",
      "0.5068194583118507\n",
      "0.4782269383961471\n",
      "0.5091430518161216\n",
      "0.5278365104527282\n",
      "0.6257695746503632\n",
      "0.4265032236054483\n",
      "0.2995252702349784\n",
      "0.5168018243676519\n",
      "0.5204968043054444\n",
      "0.49899354017133773\n",
      "0.6213564782273064\n",
      "0.5191131015109417\n",
      "0.38638767190399465\n",
      "0.5403809511321114\n",
      "0.5188720861297249\n",
      "0.5375569899715312\n",
      "0.5455411411942211\n",
      "0.5488430986640582\n",
      "0.5327416011415907\n",
      "0.617932356423037\n",
      "0.5664594733737103\n",
      "0.5846796989743303\n",
      "0.6324485085427249\n",
      "0.6470831486088862\n",
      "0.6470831486088862\n",
      "0.7715633879897844\n",
      "0.76363336484621\n",
      "0.7627262008448479\n",
      "0.6934094321322601\n",
      "0.6605871287373863\n",
      "0.5378131675618848\n",
      "0.6100493289168182\n",
      "0.6429737807068577\n",
      "0.737541295886676\n",
      "0.6717380040238027\n",
      "0.6610161997204765\n",
      "0.6989439108559212\n",
      "0.5354330697809674\n",
      "0.5363221445344158\n",
      "0.5293618786486851\n",
      "0.5464296088770993\n",
      "0.6731845571493713\n",
      "0.6540416955747477\n",
      "0.658339245135378\n",
      "0.6330039243954053\n",
      "0.6102903479665651\n",
      "0.5503083281792523\n",
      "0.5085221210060691\n",
      "0.6689326834001137\n",
      "0.6990904724538196\n",
      "0.7107237567397279\n",
      "0.5975076392415135\n",
      "0.5646114912753504\n",
      "0.6353463376257442\n",
      "0.6825776650835482\n",
      "0.5486771452551912\n",
      "0.4774705631577063\n",
      "0.6692016926610105\n",
      "0.44845625634733605\n",
      "0.6962839676855636\n",
      "0.5637354379528565\n",
      "0.5907493067875872\n",
      "0.6473680437271192\n",
      "0.5232087637389363\n",
      "0.5998878894975763\n",
      "0.722312863070603\n",
      "0.5468034670969749\n",
      "0.4459123423035023\n",
      "0.6263150697694899\n",
      "0.5209881620447767\n",
      "0.5807279526775825\n",
      "0.6462771255267766\n",
      "0.6088825608902985\n",
      "0.5706332512879024\n",
      "0.44784284678586805\n",
      "0.5771514966106445\n",
      "0.5716389130434534\n",
      "0.46396657790948437\n",
      "0.46588376343465265\n",
      "0.3960313035830354\n",
      "0.39622153883905903\n",
      "0.5800214062937403\n",
      "0.46301854583259766\n",
      "0.5079765121663378\n",
      "0.6541772675570093\n",
      "0.674692491094107\n",
      "0.7421493927936521\n",
      "0.670446466236291\n",
      "0.5122707703417245\n",
      "0.5090365100963665\n",
      "0.5784881303994238\n",
      "0.6518578875489548\n",
      "0.5415647844775521\n",
      "0.5818002413373471\n",
      "0.5757115066059781\n",
      "0.5116318700141982\n",
      "0.4155831735714008\n",
      "0.3557406684191122\n",
      "0.5329086513552197\n",
      "0.4541833063301323\n",
      "0.1313990720716061\n",
      "0.44477484921539495\n",
      "0.2745323214829851\n",
      "0.3907025677745024\n",
      "0.41893476057220524\n",
      "0.4786757425415993\n",
      "0.37613242334114405\n",
      "0.23784448088092455\n",
      "0.345075989693942\n",
      "0.4656481419789516\n",
      "0.36493641303124796\n",
      "0.45074460474962386\n",
      "0.26291640236998953\n",
      "0.19463840562133558\n",
      "0.3372208982697022\n",
      "0.44830784552158387\n",
      "0.4394718761671951\n",
      "0.395904655969549\n",
      "0.37077503316746135\n",
      "0.5241505915323285\n",
      "0.46442779217853913\n",
      "0.45391189056861314\n",
      "0.5126280256721686\n",
      "0.4305104542234424\n",
      "0.5629298065503006\n",
      "0.518529709057846\n",
      "0.6300134353078332\n",
      "0.6039373901865983\n",
      "0.7345655530915757\n",
      "0.7345655530915757\n",
      "0.6297532435058432\n",
      "0.4477844728980268\n",
      "0.48830765047781366\n",
      "0.4948863436353019\n",
      "0.6224089128408757\n",
      "0.5450420535212852\n",
      "0.62926367254234\n",
      "0.6056628751375327\n",
      "0.4874483429818634\n",
      "0.5218589972448132\n",
      "0.5918286070852403\n",
      "0.5692951755721707\n",
      "0.6581179948704905\n",
      "0.510291827547942\n",
      "0.5718989254320606\n",
      "0.6581179948704905\n",
      "0.25025327615150583\n",
      "0.2910017277747807\n",
      "0.2855835843552256\n",
      "0.2489262567091627\n",
      "0.3753117416081457\n",
      "0.3165040351111658\n",
      "0.3913728874296686\n",
      "0.485377725971798\n",
      "0.5310091396399128\n",
      "0.4247335048587341\n",
      "0.2997535596277888\n",
      "0.3624886445642946\n",
      "0.4096833229633611\n",
      "0.31103957204730515\n",
      "0.3624228325798754\n",
      "0.2905468385347613\n",
      "0.30915781603089426\n",
      "0.294934423438238\n",
      "0.20638452190631976\n",
      "0.3500615123070536\n",
      "0.42541989726092766\n",
      "0.3414680371972835\n",
      "0.24505737527112645\n",
      "0.31364306361261984\n",
      "0.26855181645765275\n",
      "0.446498662032543\n",
      "0.48248520890288155\n",
      "0.39997995646420664\n",
      "0.3023260679834435\n",
      "0.21614301766929708\n",
      "0.34092659008548715\n",
      "0.42797902706853863\n",
      "0.39030897389591007\n",
      "0.24776750786090043\n",
      "0.3531208163986832\n",
      "0.22360942077682813\n",
      "0.4241656975174139\n",
      "0.322929244594128\n",
      "0.43258797878715033\n",
      "0.41996081409293334\n",
      "0.4448558046374049\n",
      "0.5333830858059087\n",
      "0.4814010734272307\n",
      "0.22969827557305122\n",
      "0.3731054252764113\n",
      "0.27073548344944265\n",
      "0.3602085008109023\n",
      "0.37463252080577975\n",
      "0.3751628034341405\n",
      "0.277407978458921\n",
      "0.6199655891157474\n",
      "0.49636072033842177\n",
      "0.26403563266737534\n",
      "0.3442113239950178\n",
      "0.32979360841051203\n",
      "0.1723024750387811\n",
      "0.331117399177889\n",
      "0.26469225108641364\n",
      "0.34041740643185403\n",
      "0.2995702135612287\n",
      "0.3429411296202748\n",
      "0.22740666762779896\n",
      "0.18536306034259087\n",
      "0.4194957137951007\n",
      "0.5120784028199667\n",
      "0.4469342074738178\n",
      "0.6114562767657579\n",
      "0.4415528737752135\n",
      "0.4056594735566172\n",
      "0.4780318365670971\n",
      "0.382672568495901\n",
      "0.45082062841106546\n",
      "0.33812298399242874\n",
      "0.531285332573575\n",
      "0.4231098351033435\n",
      "0.49949009291140023\n",
      "0.5527951466185844\n",
      "0.44463302665514587\n",
      "0.43318970633140713\n",
      "0.44463302665514587\n",
      "0.43318970633140713\n",
      "0.6763852027219268\n",
      "0.7889053406322519\n",
      "0.5298101672085846\n",
      "0.5435225884132094\n",
      "0.5701402565812299\n",
      "0.5112764093591559\n",
      "0.4887651741032095\n",
      "0.5112764093591559\n",
      "0.9137996805364357\n",
      "0.8029063623827515\n",
      "0.8499377768306708\n",
      "0.8361805223801411\n",
      "0.42107409727160544\n",
      "0.4055948869108559\n",
      "0.43534427470807907\n",
      "0.43719731173098986\n",
      "0.5316448168361961\n",
      "0.4594679062385312\n",
      "0.7477666611353464\n",
      "0.5557994610410264\n",
      "0.21992856501590718\n",
      "0.2686916148935155\n",
      "0.2923755349284434\n",
      "0.4623746359164005\n",
      "0.45713311837908865\n",
      "0.42287530915213833\n",
      "0.4802865105205331\n",
      "0.3601558922963942\n",
      "0.3025622398595066\n",
      "0.3968527442172789\n",
      "0.19310008023233985\n",
      "0.3979539260306773\n",
      "0.3548035600706476\n",
      "0.4748012486901974\n",
      "0.30880244779309796\n",
      "0.3146610474753887\n",
      "0.28367144053615806\n",
      "0.21455282584150207\n",
      "0.17159228573419744\n",
      "0.2199755321820088\n",
      "0.33408084195949683\n",
      "0.28328892259649463\n",
      "0.26057494210169235\n",
      "0.43608630740390597\n",
      "0.32309565354216285\n",
      "0.47744793312221434\n",
      "0.40968962203683795\n",
      "0.4824855166935855\n",
      "0.5014439054626124\n",
      "0.4168572694242278\n",
      "0.44042997080876667\n",
      "0.5659827437305637\n",
      "0.6213120045571179\n",
      "0.46038806326680215\n",
      "0.4512139488051937\n",
      "0.5272503946553077\n",
      "0.5349897958877537\n",
      "0.4924694050746168\n",
      "0.2984548669402174\n",
      "0.5445880742336974\n",
      "0.5147084559827043\n",
      "0.564255133424759\n",
      "0.6685437647197906\n",
      "0.3424115311330801\n",
      "0.5261980664244044\n",
      "0.24243578636977972\n",
      "0.481513367050953\n",
      "0.2893983251580327\n",
      "0.4065629887917494\n",
      "0.27850373004504314\n",
      "0.31814171915993067\n",
      "0.30870831500800516\n",
      "0.3756124449679503\n",
      "0.4145361895416857\n",
      "0.3378954106904849\n",
      "0.23557756340215882\n",
      "0.41577386136256833\n",
      "0.3713226615613309\n",
      "0.4037717098077851\n",
      "0.353966714058004\n",
      "0.38365683874020395\n",
      "0.3927011030807215\n",
      "0.28722105514558965\n",
      "0.4276293836930055\n",
      "0.4520248540917344\n",
      "0.32614867631947786\n",
      "0.4914625141625615\n",
      "0.36341072332636176\n",
      "0.32895206784096137\n",
      "0.4380572957726214\n",
      "0.508410202256082\n",
      "0.36387155653352327\n",
      "0.34916235710242716\n",
      "0.42438886955544214\n",
      "0.4842017372007039\n",
      "0.39237388679917024\n",
      "0.42438886955544214\n",
      "0.43407563226737395\n",
      "0.4538101853436941\n",
      "0.5648257449945618\n",
      "0.4984066140086946\n",
      "0.5982127024618733\n",
      "0.552359012347202\n",
      "0.4912253468175543\n",
      "0.4342132163882951\n",
      "0.49953506321290453\n",
      "0.49953506321290453\n",
      "0.6210732307580229\n",
      "0.5045353958961767\n",
      "0.4601466722502924\n",
      "0.45910833332241446\n",
      "0.49788967524412514\n",
      "0.5198494817924509\n",
      "0.5758587814456926\n",
      "0.3888584400797285\n",
      "0.5009130162138133\n",
      "0.39907434162498534\n",
      "0.5220503014100939\n",
      "0.5703594888566987\n",
      "0.4766480420129572\n",
      "0.3849143104776468\n",
      "0.3960553160067933\n",
      "0.279269450183023\n",
      "0.28311079229108105\n",
      "0.44900922631034607\n",
      "0.4646411958059515\n",
      "0.3751608550526318\n",
      "0.34256594639392624\n",
      "0.4037863782309025\n",
      "0.39567391304145\n",
      "0.49836566005597666\n",
      "0.3591758138267789\n",
      "0.32137071595139655\n",
      "0.3928268433796122\n",
      "0.4205375728829607\n",
      "0.3765229390807923\n",
      "0.38590197634548207\n",
      "0.3385701032213306\n",
      "0.3856962502034408\n",
      "0.3394077204119828\n",
      "0.39427871053268565\n",
      "0.42610487574879985\n",
      "0.3795388795282477\n",
      "0.3548840176499522\n",
      "0.4716732418727632\n",
      "0.3637247710805494\n",
      "0.4356226034204384\n",
      "0.4736196445611134\n",
      "0.42088738089870337\n",
      "0.32162832020126697\n",
      "0.3265656393619096\n",
      "0.46704838014296696\n",
      "0.4602810416988038\n",
      "0.38372729263109223\n",
      "0.40283663852393\n",
      "0.3661237863393479\n",
      "0.3878600348296897\n",
      "0.5613300600532266\n",
      "0.3414026930899632\n",
      "0.40651544513885923\n",
      "0.49705382465705067\n",
      "0.39281198095413283\n",
      "0.5193457696291436\n",
      "0.5541863103568512\n",
      "0.38443410545874923\n",
      "0.34225436379316126\n",
      "0.33031176085531583\n",
      "0.48363505718161487\n",
      "0.3898131980453887\n",
      "0.4104168341787387\n",
      "0.2177796212439096\n",
      "0.4398388112616594\n",
      "0.34513264292334056\n",
      "0.3799266904849989\n",
      "0.45843682384886747\n",
      "0.46454771743688733\n",
      "0.19705686024044805\n",
      "0.5352267077979971\n",
      "0.3229844256219658\n",
      "0.40286714654786315\n",
      "0.4637898235108609\n",
      "0.3588601900203444\n",
      "0.42534223302237845\n",
      "0.29467643495903867\n",
      "0.519221655048042\n",
      "0.5238943775290225\n",
      "0.33530911946388653\n",
      "0.6337720468858555\n",
      "0.41709509838994785\n",
      "0.3400613054440229\n",
      "0.4057039425891287\n",
      "0.4053146734474022\n",
      "0.4213062804804695\n",
      "0.4344778924699401\n",
      "0.43764832228379563\n",
      "0.5476600949168936\n",
      "0.5437395092805745\n",
      "0.45105907864489025\n",
      "0.5008191054617772\n",
      "0.5228107194746732\n",
      "0.5634949881969007\n",
      "0.5096536573175265\n",
      "0.6691757858198848\n",
      "0.5712769885104756\n",
      "0.561559564185271\n",
      "0.603871413341945\n",
      "0.4274600929525785\n",
      "0.547765920438489\n",
      "0.5438089313108988\n",
      "0.5541833578129934\n",
      "0.5123346714238987\n",
      "0.679865695792262\n",
      "0.602293696898946\n",
      "0.5086642897326145\n",
      "0.5359164012770169\n",
      "0.38055484640576426\n",
      "0.3679616388784584\n",
      "0.3788198551284069\n",
      "0.49567462174669485\n",
      "0.5658724011157721\n",
      "0.619516186659192\n",
      "0.4833743126865587\n",
      "0.344094253294232\n",
      "0.34396274149779854\n",
      "0.31003450989597264\n",
      "0.48016365434203934\n",
      "0.46893872478134296\n",
      "0.3695210675996087\n",
      "0.02836759736870549\n",
      "0.10934450810836689\n",
      "0.10248914800560567\n",
      "0.13061040065303262\n",
      "0.3594745005535683\n",
      "0.2632752125377259\n",
      "0.2620033962640208\n",
      "0.404896115218555\n",
      "0.48669756431660793\n",
      "0.3067487693669219\n",
      "0.31903809210268946\n",
      "0.33901600008546484\n",
      "0.2462647269453796\n",
      "0.36110610515109426\n",
      "0.4612982378245061\n",
      "0.42366446577019345\n",
      "0.31746448960668505\n",
      "0.42961756456404615\n",
      "0.312344071640468\n",
      "0.4643442806365127\n",
      "0.4472326295171404\n",
      "0.23005103383122505\n",
      "0.4363655527452821\n",
      "0.3564960146004962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37461922972225536\n",
      "0.41884557142796414\n",
      "0.4258414184668856\n",
      "0.23985604187472426\n",
      "0.30524695243717337\n",
      "0.21543410163237278\n",
      "0.41406894824388557\n",
      "0.22733965224664418\n",
      "0.33212063230409494\n",
      "0.5261304718276593\n",
      "0.4478724342257041\n",
      "0.4421290735878343\n",
      "0.5487742661873499\n",
      "0.42173423902329815\n",
      "0.42385692220796267\n",
      "0.41703392939443457\n",
      "0.44771757892041913\n",
      "0.4080510947097067\n",
      "0.46591339555457323\n",
      "0.29869724959001975\n"
     ]
    }
   ],
   "source": [
    "for i , val in enumerate(norm_X2):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dfb952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1919\n"
     ]
    }
   ],
   "source": [
    "print(len(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ec99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(F[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df572c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-621.6148071289062 117.62362670898438 11.537063598632812\n",
      " 21.561647415161133 11.041670799255371 7.499429225921631\n",
      " -12.75304889678955 4.156369686126709 -5.7044477462768555\n",
      " -2.476719856262207 -8.903058052062988 4.459834098815918\n",
      " -3.821042060852051 -3.6995909214019775 1.5087281465530396\n",
      " -3.3966598510742188 -3.4508140087127686 -3.2805581092834473\n",
      " -1.827723503112793 -4.184768199920654 -1.158416986465454\n",
      " -1.680070400238037 -3.0591936111450195 -0.07024428248405457\n",
      " -0.16802792251110077 -1.6682277917861938 -0.5772632956504822\n",
      " -1.1854113340377808 -0.9187160134315491 -2.9014546871185303\n",
      " -1.7659615278244019 -1.8429491519927979 -1.4652388095855713\n",
      " -2.1726138591766357 -0.5401995778083801 -1.7346760034561157\n",
      " -1.0902279615402222 -1.9274250268936157 -1.6069029569625854\n",
      " -1.7990167140960693 0.4838631990707972]\n"
     ]
    }
   ],
   "source": [
    "print((F[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dadf752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(F):\n",
    "    print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9063dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(F):\n",
    "    maximum_len  = len(val)\n",
    "    if len(F[i]) > maximum_len:\n",
    "        maximum_len  = len(val)\n",
    "print(maximum_len)\n",
    "#     print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e65eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X)\n",
    "# print(len(X))\n",
    "# print(len(X[0]))\n",
    "# print(\"--------------\")\n",
    "# print(X2)\n",
    "# print(len(X2))\n",
    "# print(len(X2[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8352883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilal\n",
      "Bilal\n",
      "Bilal\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# 0 till 50 one class.\n",
    "print(y[0])\n",
    "print(y[1])\n",
    "print(y[50])\n",
    "\n",
    "\n",
    "print(yy[0])\n",
    "print(yy[1])\n",
    "print(yy[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b40767d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1919, 19)\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(yy.shape)\n",
    "print(yy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59ee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(F, yy, test_size=0.2, random_state = 42)\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics \n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_shape=(41,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c0a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-409.98236083984375 113.35786437988281 -23.51805877685547\n",
      " 6.979268550872803 -18.364120483398438 -12.62409782409668\n",
      " -26.66593360900879 -22.433067321777344 -5.113967418670654\n",
      " -8.449514389038086 -12.664848327636719 -11.04189682006836\n",
      " -6.0643157958984375 -4.867355823516846 -11.413847923278809\n",
      " -8.391881942749023 -1.9053698778152466 -11.355560302734375\n",
      " -7.219545364379883 0.24228471517562866 -1.2177953720092773\n",
      " 0.5420490503311157 3.707319974899292 6.1955366134643555\n",
      " 2.8602449893951416 4.571466445922852 1.9503637552261353\n",
      " -1.7155025005340576 2.8855886459350586 3.504944324493408\n",
      " 2.4413211345672607 1.8616468906402588 2.5968708992004395\n",
      " 0.9113388061523438 -0.0389900803565979 -0.8537790775299072\n",
      " -0.08357091248035431 -1.2366302013397217 -0.7334344387054443\n",
      " 0.22933459281921387 0.45074460474962386]\n",
      "1535\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(np.shape(x_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7489cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583e7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6efaa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compilation , specify loss , optimizer and metric\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0993c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "1535\n",
      "1919\n"
     ]
    }
   ],
   "source": [
    "# for i, val in enumerate(x_test):\n",
    "#     print()\n",
    "print(len(x_test))\n",
    "print(len(x_train))\n",
    "print(len(x_test) + len(x_train) )\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b10705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.09982361e+02  1.13357864e+02 -2.35180588e+01  6.97926855e+00\n",
      " -1.83641205e+01 -1.26240978e+01 -2.66659336e+01 -2.24330673e+01\n",
      " -5.11396742e+00 -8.44951439e+00 -1.26648483e+01 -1.10418968e+01\n",
      " -6.06431580e+00 -4.86735582e+00 -1.14138479e+01 -8.39188194e+00\n",
      " -1.90536988e+00 -1.13555603e+01 -7.21954536e+00  2.42284715e-01\n",
      " -1.21779537e+00  5.42049050e-01  3.70731997e+00  6.19553661e+00\n",
      "  2.86024499e+00  4.57146645e+00  1.95036376e+00 -1.71550250e+00\n",
      "  2.88558865e+00  3.50494432e+00  2.44132113e+00  1.86164689e+00\n",
      "  2.59687090e+00  9.11338806e-01 -3.89900804e-02 -8.53779078e-01\n",
      " -8.35709125e-02 -1.23663020e+00 -7.33434439e-01  2.29334593e-01\n",
      "  4.50744599e-01]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8986664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               10752     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 19)                4883      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 19)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,427\n",
      "Trainable params: 81,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 35.0311 - accuracy: 0.0078\n",
      "Pre-training accuracy: 0.7812%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e06fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 51.6224 - accuracy: 0.0682\n",
      "Epoch 00001: val_loss improved from inf to 2.71233, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 1s 5ms/step - loss: 42.1107 - accuracy: 0.0736 - val_loss: 2.7123 - val_accuracy: 0.1250\n",
      "Epoch 2/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 8.7414 - accuracy: 0.0793 \n",
      "Epoch 00002: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 7.9095 - accuracy: 0.0866 - val_loss: 2.9082 - val_accuracy: 0.0833\n",
      "Epoch 3/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 3.7165 - accuracy: 0.0938\n",
      "Epoch 00003: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 3.6635 - accuracy: 0.0945 - val_loss: 2.9123 - val_accuracy: 0.0859\n",
      "Epoch 4/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 3.0917 - accuracy: 0.1071  \n",
      "Epoch 00004: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 3.0781 - accuracy: 0.1016 - val_loss: 2.8749 - val_accuracy: 0.1016\n",
      "Epoch 5/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 3.0150 - accuracy: 0.1039\n",
      "Epoch 00005: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 3.0300 - accuracy: 0.1016 - val_loss: 2.8168 - val_accuracy: 0.1094\n",
      "Epoch 6/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 2.9160 - accuracy: 0.1148\n",
      "Epoch 00006: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.9261 - accuracy: 0.1114 - val_loss: 2.7996 - val_accuracy: 0.1094\n",
      "Epoch 7/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 2.8899 - accuracy: 0.1122\n",
      "Epoch 00007: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.8974 - accuracy: 0.1134 - val_loss: 2.7870 - val_accuracy: 0.0938\n",
      "Epoch 8/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 2.8804 - accuracy: 0.0929\n",
      "Epoch 00008: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.8678 - accuracy: 0.0925 - val_loss: 2.7750 - val_accuracy: 0.1094\n",
      "Epoch 9/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 2.8362 - accuracy: 0.1184\n",
      "Epoch 00009: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.8243 - accuracy: 0.1140 - val_loss: 2.7704 - val_accuracy: 0.0938\n",
      "Epoch 10/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 2.8327 - accuracy: 0.1181\n",
      "Epoch 00010: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 2.8297 - accuracy: 0.1166 - val_loss: 2.7686 - val_accuracy: 0.0938\n",
      "Epoch 11/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 2.8400 - accuracy: 0.1065\n",
      "Epoch 00011: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.8502 - accuracy: 0.1068 - val_loss: 2.7662 - val_accuracy: 0.0938\n",
      "Epoch 12/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 2.8013 - accuracy: 0.1278\n",
      "Epoch 00012: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.8060 - accuracy: 0.1192 - val_loss: 2.7640 - val_accuracy: 0.0990\n",
      "Epoch 13/200\n",
      "32/48 [===================>..........] - ETA: 0s - loss: 2.7688 - accuracy: 0.1250\n",
      "Epoch 00013: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 2.7871 - accuracy: 0.1166 - val_loss: 2.7630 - val_accuracy: 0.1016\n",
      "Epoch 14/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 2.7902 - accuracy: 0.1125\n",
      "Epoch 00014: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.7974 - accuracy: 0.1127 - val_loss: 2.7360 - val_accuracy: 0.1328\n",
      "Epoch 15/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 2.7616 - accuracy: 0.1143\n",
      "Epoch 00015: val_loss did not improve from 2.71233\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.7602 - accuracy: 0.1153 - val_loss: 2.7607 - val_accuracy: 0.0990\n",
      "Epoch 16/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 2.7808 - accuracy: 0.1243\n",
      "Epoch 00016: val_loss improved from 2.71233 to 2.63766, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.7801 - accuracy: 0.1251 - val_loss: 2.6377 - val_accuracy: 0.1328\n",
      "Epoch 17/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 2.7470 - accuracy: 0.1292\n",
      "Epoch 00017: val_loss did not improve from 2.63766\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 2.7435 - accuracy: 0.1303 - val_loss: 2.6904 - val_accuracy: 0.1354\n",
      "Epoch 18/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 2.7288 - accuracy: 0.1386\n",
      "Epoch 00018: val_loss improved from 2.63766 to 2.59523, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.7301 - accuracy: 0.1368 - val_loss: 2.5952 - val_accuracy: 0.1406\n",
      "Epoch 19/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 2.6847 - accuracy: 0.1467\n",
      "Epoch 00019: val_loss improved from 2.59523 to 2.56760, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.6931 - accuracy: 0.1414 - val_loss: 2.5676 - val_accuracy: 0.1458\n",
      "Epoch 20/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 2.6532 - accuracy: 0.1548\n",
      "Epoch 00020: val_loss improved from 2.56760 to 2.55659, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.6565 - accuracy: 0.1537 - val_loss: 2.5566 - val_accuracy: 0.1745\n",
      "Epoch 21/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 2.6866 - accuracy: 0.1303\n",
      "Epoch 00021: val_loss improved from 2.55659 to 2.55103, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.6768 - accuracy: 0.1355 - val_loss: 2.5510 - val_accuracy: 0.2135\n",
      "Epoch 22/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 2.6185 - accuracy: 0.1556\n",
      "Epoch 00022: val_loss improved from 2.55103 to 2.41497, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 2.6106 - accuracy: 0.1577 - val_loss: 2.4150 - val_accuracy: 0.2161\n",
      "Epoch 23/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 2.5724 - accuracy: 0.1443 ETA: 0s - loss: 2.6176 - accuracy: 0.\n",
      "Epoch 00023: val_loss improved from 2.41497 to 2.30390, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 2.5595 - accuracy: 0.1498 - val_loss: 2.3039 - val_accuracy: 0.1927\n",
      "Epoch 24/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 2.5778 - accuracy: 0.1439\n",
      "Epoch 00024: val_loss improved from 2.30390 to 2.30112, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 2.5615 - accuracy: 0.1433 - val_loss: 2.3011 - val_accuracy: 0.2083\n",
      "Epoch 25/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 2.5120 - accuracy: 0.1615\n",
      "Epoch 00025: val_loss improved from 2.30112 to 2.16346, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.4887 - accuracy: 0.1661 - val_loss: 2.1635 - val_accuracy: 0.2865\n",
      "Epoch 26/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 2.4315 - accuracy: 0.1977\n",
      "Epoch 00026: val_loss improved from 2.16346 to 2.09481, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.4361 - accuracy: 0.1954 - val_loss: 2.0948 - val_accuracy: 0.2526\n",
      "Epoch 27/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 2.3962 - accuracy: 0.1937\n",
      "Epoch 00027: val_loss improved from 2.09481 to 2.08176, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 9ms/step - loss: 2.4033 - accuracy: 0.1922 - val_loss: 2.0818 - val_accuracy: 0.2865\n",
      "Epoch 28/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 2.2857 - accuracy: 0.2163\n",
      "Epoch 00028: val_loss improved from 2.08176 to 1.95501, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 2.2869 - accuracy: 0.2169 - val_loss: 1.9550 - val_accuracy: 0.2995\n",
      "Epoch 29/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 2.2337 - accuracy: 0.2246\n",
      "Epoch 00029: val_loss improved from 1.95501 to 1.89290, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2385 - accuracy: 0.2248 - val_loss: 1.8929 - val_accuracy: 0.3516\n",
      "Epoch 30/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 2.1709 - accuracy: 0.2604\n",
      "Epoch 00030: val_loss improved from 1.89290 to 1.77983, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 2.1795 - accuracy: 0.2560 - val_loss: 1.7798 - val_accuracy: 0.3802\n",
      "Epoch 31/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 2.1192 - accuracy: 0.2717\n",
      "Epoch 00031: val_loss improved from 1.77983 to 1.75050, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 2.1164 - accuracy: 0.2730 - val_loss: 1.7505 - val_accuracy: 0.4036\n",
      "Epoch 32/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 2.0548 - accuracy: 0.3045\n",
      "Epoch 00032: val_loss improved from 1.75050 to 1.55981, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 2.0523 - accuracy: 0.2958 - val_loss: 1.5598 - val_accuracy: 0.5495\n",
      "Epoch 33/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 2.0086 - accuracy: 0.3352\n",
      "Epoch 00033: val_loss improved from 1.55981 to 1.48178, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 2.0185 - accuracy: 0.3336 - val_loss: 1.4818 - val_accuracy: 0.5833\n",
      "Epoch 34/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 1.8815 - accuracy: 0.3516\n",
      "Epoch 00034: val_loss improved from 1.48178 to 1.33663, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 1.8444 - accuracy: 0.3635 - val_loss: 1.3366 - val_accuracy: 0.6198\n",
      "Epoch 35/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 1.7594 - accuracy: 0.4012\n",
      "Epoch 00035: val_loss improved from 1.33663 to 1.25591, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 1.7548 - accuracy: 0.4013 - val_loss: 1.2559 - val_accuracy: 0.6589\n",
      "Epoch 36/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 1.6768 - accuracy: 0.4240\n",
      "Epoch 00036: val_loss improved from 1.25591 to 1.17287, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.6701 - accuracy: 0.4319 - val_loss: 1.1729 - val_accuracy: 0.6823\n",
      "Epoch 37/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 1.5610 - accuracy: 0.4696\n",
      "Epoch 00037: val_loss improved from 1.17287 to 1.10246, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 1.5456 - accuracy: 0.4795 - val_loss: 1.1025 - val_accuracy: 0.7005\n",
      "Epoch 38/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 1.4800 - accuracy: 0.5107\n",
      "Epoch 00038: val_loss improved from 1.10246 to 0.86606, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.4527 - accuracy: 0.5173 - val_loss: 0.8661 - val_accuracy: 0.7734\n",
      "Epoch 39/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 1.3031 - accuracy: 0.5701\n",
      "Epoch 00039: val_loss improved from 0.86606 to 0.69222, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.3031 - accuracy: 0.5779 - val_loss: 0.6922 - val_accuracy: 0.8672\n",
      "Epoch 40/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 1.1661 - accuracy: 0.6102\n",
      "Epoch 00040: val_loss improved from 0.69222 to 0.63448, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.1503 - accuracy: 0.6091 - val_loss: 0.6345 - val_accuracy: 0.8464\n",
      "Epoch 41/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 0.9540 - accuracy: 0.6695\n",
      "Epoch 00041: val_loss improved from 0.63448 to 0.48664, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.9426 - accuracy: 0.6782 - val_loss: 0.4866 - val_accuracy: 0.8932\n",
      "Epoch 42/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 0.8664 - accuracy: 0.7140\n",
      "Epoch 00042: val_loss improved from 0.48664 to 0.43525, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.8766 - accuracy: 0.7127 - val_loss: 0.4353 - val_accuracy: 0.8932\n",
      "Epoch 43/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.7814 - accuracy: 0.7466\n",
      "Epoch 00043: val_loss improved from 0.43525 to 0.35949, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.7688 - accuracy: 0.7485 - val_loss: 0.3595 - val_accuracy: 0.9245\n",
      "Epoch 44/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.7075 - accuracy: 0.7656\n",
      "Epoch 00044: val_loss did not improve from 0.35949\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.7029 - accuracy: 0.7681 - val_loss: 0.3695 - val_accuracy: 0.9193\n",
      "Epoch 45/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 0.6925 - accuracy: 0.7625\n",
      "Epoch 00045: val_loss improved from 0.35949 to 0.34140, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.7759 - val_loss: 0.3414 - val_accuracy: 0.9167\n",
      "Epoch 46/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.6597 - accuracy: 0.7812\n",
      "Epoch 00046: val_loss improved from 0.34140 to 0.28028, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.6441 - accuracy: 0.7857 - val_loss: 0.2803 - val_accuracy: 0.9427\n",
      "Epoch 47/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.5904 - accuracy: 0.8049\n",
      "Epoch 00047: val_loss improved from 0.28028 to 0.27889, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5824 - accuracy: 0.8085 - val_loss: 0.2789 - val_accuracy: 0.9375\n",
      "Epoch 48/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.5284 - accuracy: 0.8367\n",
      "Epoch 00048: val_loss improved from 0.27889 to 0.24186, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.5469 - accuracy: 0.8306 - val_loss: 0.2419 - val_accuracy: 0.9427\n",
      "Epoch 49/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.5013 - accuracy: 0.8510\n",
      "Epoch 00049: val_loss did not improve from 0.24186\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.4942 - accuracy: 0.8541 - val_loss: 0.2468 - val_accuracy: 0.9479\n",
      "Epoch 50/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.4220 - accuracy: 0.8685\n",
      "Epoch 00050: val_loss improved from 0.24186 to 0.23843, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.4424 - accuracy: 0.8625 - val_loss: 0.2384 - val_accuracy: 0.9453\n",
      "Epoch 51/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.4480 - accuracy: 0.8508\n",
      "Epoch 00051: val_loss improved from 0.23843 to 0.20006, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4435 - accuracy: 0.8567 - val_loss: 0.2001 - val_accuracy: 0.9479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.3945 - accuracy: 0.8729\n",
      "Epoch 00052: val_loss improved from 0.20006 to 0.18825, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3980 - accuracy: 0.8730 - val_loss: 0.1882 - val_accuracy: 0.9557\n",
      "Epoch 53/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3718 - accuracy: 0.8854\n",
      "Epoch 00053: val_loss did not improve from 0.18825\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3656 - accuracy: 0.8873 - val_loss: 0.2013 - val_accuracy: 0.9453\n",
      "Epoch 54/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.3410 - accuracy: 0.8899\n",
      "Epoch 00054: val_loss did not improve from 0.18825\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3427 - accuracy: 0.8893 - val_loss: 0.1908 - val_accuracy: 0.9427\n",
      "Epoch 55/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3286 - accuracy: 0.8947\n",
      "Epoch 00055: val_loss improved from 0.18825 to 0.16504, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3293 - accuracy: 0.8932 - val_loss: 0.1650 - val_accuracy: 0.9531\n",
      "Epoch 56/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.3392 - accuracy: 0.8934\n",
      "Epoch 00056: val_loss did not improve from 0.16504\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3320 - accuracy: 0.9003 - val_loss: 0.1724 - val_accuracy: 0.9557\n",
      "Epoch 57/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.2892 - accuracy: 0.9109\n",
      "Epoch 00057: val_loss improved from 0.16504 to 0.16406, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2833 - accuracy: 0.9088 - val_loss: 0.1641 - val_accuracy: 0.9609\n",
      "Epoch 58/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.2632 - accuracy: 0.9084\n",
      "Epoch 00058: val_loss improved from 0.16406 to 0.15428, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2744 - accuracy: 0.9068 - val_loss: 0.1543 - val_accuracy: 0.9531\n",
      "Epoch 59/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.3040 - accuracy: 0.9119\n",
      "Epoch 00059: val_loss improved from 0.15428 to 0.14813, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2958 - accuracy: 0.9134 - val_loss: 0.1481 - val_accuracy: 0.9557\n",
      "Epoch 60/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.2612 - accuracy: 0.9055\n",
      "Epoch 00060: val_loss improved from 0.14813 to 0.14778, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2654 - accuracy: 0.9049 - val_loss: 0.1478 - val_accuracy: 0.9557\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9270\n",
      "Epoch 00061: val_loss improved from 0.14778 to 0.14540, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2536 - accuracy: 0.9270 - val_loss: 0.1454 - val_accuracy: 0.9557\n",
      "Epoch 62/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 0.2503 - accuracy: 0.9169\n",
      "Epoch 00062: val_loss did not improve from 0.14540\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2423 - accuracy: 0.9218 - val_loss: 0.1469 - val_accuracy: 0.9609\n",
      "Epoch 63/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.2512 - accuracy: 0.9206\n",
      "Epoch 00063: val_loss did not improve from 0.14540\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2718 - accuracy: 0.9173 - val_loss: 0.1661 - val_accuracy: 0.9557\n",
      "Epoch 64/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 0.2360 - accuracy: 0.9219\n",
      "Epoch 00064: val_loss did not improve from 0.14540\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9270 - val_loss: 0.1584 - val_accuracy: 0.9609\n",
      "Epoch 65/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.2071 - accuracy: 0.9318\n",
      "Epoch 00065: val_loss did not improve from 0.14540\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2140 - accuracy: 0.9309 - val_loss: 0.1604 - val_accuracy: 0.9635\n",
      "Epoch 66/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2028 - accuracy: 0.9341\n",
      "Epoch 00066: val_loss improved from 0.14540 to 0.14507, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2020 - accuracy: 0.9342 - val_loss: 0.1451 - val_accuracy: 0.9661\n",
      "Epoch 67/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.2019 - accuracy: 0.9382\n",
      "Epoch 00067: val_loss did not improve from 0.14507\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1927 - accuracy: 0.9414 - val_loss: 0.1578 - val_accuracy: 0.9609\n",
      "Epoch 68/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.2179 - accuracy: 0.9281\n",
      "Epoch 00068: val_loss did not improve from 0.14507\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.9329 - val_loss: 0.1466 - val_accuracy: 0.9661\n",
      "Epoch 69/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.2046 - accuracy: 0.9345\n",
      "Epoch 00069: val_loss improved from 0.14507 to 0.14493, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2006 - accuracy: 0.9375 - val_loss: 0.1449 - val_accuracy: 0.9609\n",
      "Epoch 70/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1876 - accuracy: 0.9455\n",
      "Epoch 00070: val_loss improved from 0.14493 to 0.11028, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.1941 - accuracy: 0.9420 - val_loss: 0.1103 - val_accuracy: 0.9714\n",
      "Epoch 71/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.9426\n",
      "Epoch 00071: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9427 - val_loss: 0.1423 - val_accuracy: 0.9635\n",
      "Epoch 72/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2011 - accuracy: 0.9354\n",
      "Epoch 00072: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2014 - accuracy: 0.9362 - val_loss: 0.1619 - val_accuracy: 0.9635\n",
      "Epoch 73/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9338\n",
      "Epoch 00073: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.9368 - val_loss: 0.1409 - val_accuracy: 0.9661\n",
      "Epoch 74/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.9469\n",
      "Epoch 00074: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.9498 - val_loss: 0.1509 - val_accuracy: 0.9661\n",
      "Epoch 75/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.2092 - accuracy: 0.9391\n",
      "Epoch 00075: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2003 - accuracy: 0.9427 - val_loss: 0.1426 - val_accuracy: 0.9635\n",
      "Epoch 76/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1617 - accuracy: 0.9528\n",
      "Epoch 00076: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.9524 - val_loss: 0.1257 - val_accuracy: 0.9714\n",
      "Epoch 77/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1624 - accuracy: 0.9451\n",
      "Epoch 00077: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1677 - accuracy: 0.9446 - val_loss: 0.1533 - val_accuracy: 0.9661\n",
      "Epoch 78/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1644 - accuracy: 0.9477\n",
      "Epoch 00078: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1612 - accuracy: 0.9485 - val_loss: 0.1329 - val_accuracy: 0.9766\n",
      "Epoch 79/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1650 - accuracy: 0.9516 ETA: 0s - loss: 0.1552 - accuracy: 0.95\n",
      "Epoch 00079: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9485 - val_loss: 0.1239 - val_accuracy: 0.9714\n",
      "Epoch 80/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1291 - accuracy: 0.9569\n",
      "Epoch 00080: val_loss did not improve from 0.11028\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9596 - val_loss: 0.1430 - val_accuracy: 0.9635\n",
      "Epoch 81/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.1336 - accuracy: 0.9581\n",
      "Epoch 00081: val_loss improved from 0.11028 to 0.10609, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9590 - val_loss: 0.1061 - val_accuracy: 0.9818\n",
      "Epoch 82/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1890 - accuracy: 0.9319\n",
      "Epoch 00082: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9349 - val_loss: 0.1274 - val_accuracy: 0.9714\n",
      "Epoch 83/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9547\n",
      "Epoch 00083: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1332 - accuracy: 0.9577 - val_loss: 0.1271 - val_accuracy: 0.9766\n",
      "Epoch 84/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1326 - accuracy: 0.9542\n",
      "Epoch 00084: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9544 - val_loss: 0.1115 - val_accuracy: 0.9661\n",
      "Epoch 85/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9650\n",
      "Epoch 00085: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9635 - val_loss: 0.1363 - val_accuracy: 0.9661\n",
      "Epoch 86/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9604\n",
      "Epoch 00086: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1315 - accuracy: 0.9590 - val_loss: 0.1101 - val_accuracy: 0.9714\n",
      "Epoch 87/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 0.1386 - accuracy: 0.9598\n",
      "Epoch 00087: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9537 - val_loss: 0.1190 - val_accuracy: 0.9792\n",
      "Epoch 88/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1103 - accuracy: 0.9695\n",
      "Epoch 00088: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9700 - val_loss: 0.1094 - val_accuracy: 0.9740\n",
      "Epoch 89/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1483 - accuracy: 0.9469\n",
      "Epoch 00089: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1506 - accuracy: 0.9492 - val_loss: 0.1416 - val_accuracy: 0.9766\n",
      "Epoch 90/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9673\n",
      "Epoch 00090: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9674 - val_loss: 0.1148 - val_accuracy: 0.9714\n",
      "Epoch 91/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1092 - accuracy: 0.9641\n",
      "Epoch 00091: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9635 - val_loss: 0.1198 - val_accuracy: 0.9740\n",
      "Epoch 92/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9616\n",
      "Epoch 00092: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 0.9616 - val_loss: 0.1254 - val_accuracy: 0.9740\n",
      "Epoch 93/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.0892 - accuracy: 0.9704\n",
      "Epoch 00093: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9726 - val_loss: 0.1292 - val_accuracy: 0.9766\n",
      "Epoch 94/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9653\n",
      "Epoch 00094: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.9661 - val_loss: 0.1317 - val_accuracy: 0.9688\n",
      "Epoch 95/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.1079 - accuracy: 0.9608\n",
      "Epoch 00095: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9609 - val_loss: 0.1464 - val_accuracy: 0.9714\n",
      "Epoch 96/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.1024 - accuracy: 0.9713\n",
      "Epoch 00096: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9707 - val_loss: 0.1252 - val_accuracy: 0.9740\n",
      "Epoch 97/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0960 - accuracy: 0.9708\n",
      "Epoch 00097: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9713 - val_loss: 0.1281 - val_accuracy: 0.9766\n",
      "Epoch 98/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9668 ETA: 0s - loss: 0.1189 - accuracy: 0.\n",
      "Epoch 00098: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.1137 - accuracy: 0.9668 - val_loss: 0.1076 - val_accuracy: 0.9792\n",
      "Epoch 99/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.0818 - accuracy: 0.9768\n",
      "Epoch 00099: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9772 - val_loss: 0.1455 - val_accuracy: 0.9635\n",
      "Epoch 100/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.1271 - accuracy: 0.9659\n",
      "Epoch 00100: val_loss did not improve from 0.10609\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.1203 - accuracy: 0.9681 - val_loss: 0.1109 - val_accuracy: 0.9818\n",
      "Epoch 101/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0911 - accuracy: 0.9715\n",
      "Epoch 00101: val_loss improved from 0.10609 to 0.09484, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0928 - accuracy: 0.9713 - val_loss: 0.0948 - val_accuracy: 0.9740\n",
      "Epoch 102/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0844 - accuracy: 0.9715\n",
      "Epoch 00102: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0865 - accuracy: 0.9713 - val_loss: 0.1018 - val_accuracy: 0.9844\n",
      "Epoch 103/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0911 - accuracy: 0.9703\n",
      "Epoch 00103: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0927 - accuracy: 0.9687 - val_loss: 0.1145 - val_accuracy: 0.9818\n",
      "Epoch 104/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0964 - accuracy: 0.9709\n",
      "Epoch 00104: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.1159 - val_accuracy: 0.9766\n",
      "Epoch 105/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 0.0902 - accuracy: 0.9705\n",
      "Epoch 00105: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0991 - accuracy: 0.9681 - val_loss: 0.1291 - val_accuracy: 0.9688\n",
      "Epoch 106/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9628\n",
      "Epoch 00106: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1258 - accuracy: 0.9622 - val_loss: 0.1131 - val_accuracy: 0.9766\n",
      "Epoch 107/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 0.0848 - accuracy: 0.9720\n",
      "Epoch 00107: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9687 - val_loss: 0.1298 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9755\n",
      "Epoch 00108: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0872 - accuracy: 0.9752 - val_loss: 0.1055 - val_accuracy: 0.9766\n",
      "Epoch 109/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.1180 - accuracy: 0.9633\n",
      "Epoch 00109: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.1194 - accuracy: 0.9635 - val_loss: 0.1150 - val_accuracy: 0.9818\n",
      "Epoch 110/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9767\n",
      "Epoch 00110: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.9765 - val_loss: 0.1120 - val_accuracy: 0.9844\n",
      "Epoch 111/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9719\n",
      "Epoch 00111: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9687 - val_loss: 0.1261 - val_accuracy: 0.9792\n",
      "Epoch 112/200\n",
      "34/48 [====================>.........] - ETA: 0s - loss: 0.1237 - accuracy: 0.9642\n",
      "Epoch 00112: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9642 - val_loss: 0.1754 - val_accuracy: 0.9583\n",
      "Epoch 113/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9694\n",
      "Epoch 00113: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9694 - val_loss: 0.1265 - val_accuracy: 0.9661\n",
      "Epoch 114/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1126 - accuracy: 0.9658\n",
      "Epoch 00114: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1111 - accuracy: 0.9661 - val_loss: 0.1208 - val_accuracy: 0.9740\n",
      "Epoch 115/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0799 - accuracy: 0.9766\n",
      "Epoch 00115: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0784 - accuracy: 0.9772 - val_loss: 0.1353 - val_accuracy: 0.9766\n",
      "Epoch 116/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.0994 - accuracy: 0.9688 ETA: 0s - loss: 0.1105 - accuracy: 0.96\n",
      "Epoch 00116: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0985 - accuracy: 0.9681 - val_loss: 0.1118 - val_accuracy: 0.9818\n",
      "Epoch 117/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.1187 - accuracy: 0.9703\n",
      "Epoch 00117: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9694 - val_loss: 0.1298 - val_accuracy: 0.9766\n",
      "Epoch 118/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0906 - accuracy: 0.9721\n",
      "Epoch 00118: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9726 - val_loss: 0.1067 - val_accuracy: 0.9844\n",
      "Epoch 119/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0996 - accuracy: 0.9688\n",
      "Epoch 00119: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0975 - accuracy: 0.9687 - val_loss: 0.1437 - val_accuracy: 0.9818\n",
      "Epoch 120/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0832 - accuracy: 0.9723\n",
      "Epoch 00120: val_loss did not improve from 0.09484\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9713 - val_loss: 0.1272 - val_accuracy: 0.9818\n",
      "Epoch 121/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.1202 - accuracy: 0.9712\n",
      "Epoch 00121: val_loss improved from 0.09484 to 0.08448, saving model to saved_models\\weights.best.basic_mlp_with_sc.hdf5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1160 - accuracy: 0.9687 - val_loss: 0.0845 - val_accuracy: 0.9818\n",
      "Epoch 122/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9654\n",
      "Epoch 00122: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1098 - accuracy: 0.9655 - val_loss: 0.1463 - val_accuracy: 0.9766\n",
      "Epoch 123/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0925 - accuracy: 0.9643\n",
      "Epoch 00123: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 0.9622 - val_loss: 0.1393 - val_accuracy: 0.9661\n",
      "Epoch 124/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.1050 - accuracy: 0.9715\n",
      "Epoch 00124: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1111 - accuracy: 0.9707 - val_loss: 0.1310 - val_accuracy: 0.9818\n",
      "Epoch 125/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0722 - accuracy: 0.9711\n",
      "Epoch 00125: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0773 - accuracy: 0.9707 - val_loss: 0.1323 - val_accuracy: 0.9844\n",
      "Epoch 126/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.0954 - accuracy: 0.9772\n",
      "Epoch 00126: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.1079 - accuracy: 0.9746 - val_loss: 0.1357 - val_accuracy: 0.9714\n",
      "Epoch 127/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1011 - accuracy: 0.9702\n",
      "Epoch 00127: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9713 - val_loss: 0.1393 - val_accuracy: 0.9740\n",
      "Epoch 128/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9721\n",
      "Epoch 00128: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.9726 - val_loss: 0.1086 - val_accuracy: 0.9792\n",
      "Epoch 129/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9701\n",
      "Epoch 00129: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9694 - val_loss: 0.1305 - val_accuracy: 0.9740\n",
      "Epoch 130/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.1039 - accuracy: 0.9703\n",
      "Epoch 00130: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9707 - val_loss: 0.1310 - val_accuracy: 0.9740\n",
      "Epoch 131/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1053 - accuracy: 0.9740\n",
      "Epoch 00131: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9752 - val_loss: 0.1343 - val_accuracy: 0.9740\n",
      "Epoch 132/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0851 - accuracy: 0.9751\n",
      "Epoch 00132: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9746 - val_loss: 0.1258 - val_accuracy: 0.9740\n",
      "Epoch 133/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0925 - accuracy: 0.9702\n",
      "Epoch 00133: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9681 - val_loss: 0.1322 - val_accuracy: 0.9740\n",
      "Epoch 134/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9767\n",
      "Epoch 00134: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9759 - val_loss: 0.1331 - val_accuracy: 0.9740\n",
      "Epoch 135/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0784 - accuracy: 0.9751\n",
      "Epoch 00135: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9759 - val_loss: 0.1343 - val_accuracy: 0.9766\n",
      "Epoch 136/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0585 - accuracy: 0.9801\n",
      "Epoch 00136: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9779 - val_loss: 0.1412 - val_accuracy: 0.9844\n",
      "Epoch 137/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0774 - accuracy: 0.9756\n",
      "Epoch 00137: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9772 - val_loss: 0.1351 - val_accuracy: 0.9740\n",
      "Epoch 138/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9628\n",
      "Epoch 00138: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9635 - val_loss: 0.1078 - val_accuracy: 0.9792\n",
      "Epoch 139/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9715\n",
      "Epoch 00139: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9726 - val_loss: 0.1254 - val_accuracy: 0.9818\n",
      "Epoch 140/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9700\n",
      "Epoch 00140: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9700 - val_loss: 0.1202 - val_accuracy: 0.9870\n",
      "Epoch 141/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0726 - accuracy: 0.9715\n",
      "Epoch 00141: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9720 - val_loss: 0.1443 - val_accuracy: 0.9844\n",
      "Epoch 142/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0937 - accuracy: 0.9656\n",
      "Epoch 00142: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0937 - accuracy: 0.9668 - val_loss: 0.1458 - val_accuracy: 0.9818\n",
      "Epoch 143/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0827 - accuracy: 0.9710\n",
      "Epoch 00143: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9713 - val_loss: 0.1501 - val_accuracy: 0.9792\n",
      "Epoch 144/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0796 - accuracy: 0.9741\n",
      "Epoch 00144: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9733 - val_loss: 0.1309 - val_accuracy: 0.9818\n",
      "Epoch 145/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 0.0615 - accuracy: 0.9812\n",
      "Epoch 00145: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9805 - val_loss: 0.1182 - val_accuracy: 0.9844\n",
      "Epoch 146/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.1302 - accuracy: 0.9603\n",
      "Epoch 00146: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9635 - val_loss: 0.1765 - val_accuracy: 0.9688\n",
      "Epoch 147/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9714\n",
      "Epoch 00147: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9713 - val_loss: 0.1671 - val_accuracy: 0.9740\n",
      "Epoch 148/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.1098 - accuracy: 0.9688\n",
      "Epoch 00148: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9694 - val_loss: 0.1190 - val_accuracy: 0.9740\n",
      "Epoch 149/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0776 - accuracy: 0.9750\n",
      "Epoch 00149: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0742 - accuracy: 0.9759 - val_loss: 0.0969 - val_accuracy: 0.9844\n",
      "Epoch 150/200\n",
      "32/48 [===================>..........] - ETA: 0s - loss: 0.0735 - accuracy: 0.9746\n",
      "Epoch 00150: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9785 - val_loss: 0.1159 - val_accuracy: 0.9870\n",
      "Epoch 151/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9792\n",
      "Epoch 00151: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.1266 - val_accuracy: 0.9818\n",
      "Epoch 152/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0615 - accuracy: 0.9784\n",
      "Epoch 00152: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9792 - val_loss: 0.1754 - val_accuracy: 0.9792\n",
      "Epoch 153/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0903 - accuracy: 0.9725\n",
      "Epoch 00153: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9726 - val_loss: 0.1578 - val_accuracy: 0.9818\n",
      "Epoch 154/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.0679 - accuracy: 0.9736\n",
      "Epoch 00154: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9759 - val_loss: 0.1338 - val_accuracy: 0.9792\n",
      "Epoch 155/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 0.0801 - accuracy: 0.9811\n",
      "Epoch 00155: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9837 - val_loss: 0.1116 - val_accuracy: 0.9818\n",
      "Epoch 156/200\n",
      "33/48 [===================>..........] - ETA: 0s - loss: 0.0915 - accuracy: 0.9725\n",
      "Epoch 00156: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9739 - val_loss: 0.1362 - val_accuracy: 0.9818\n",
      "Epoch 157/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 0.0619 - accuracy: 0.9803\n",
      "Epoch 00157: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.9785 - val_loss: 0.1429 - val_accuracy: 0.9766\n",
      "Epoch 158/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0779 - accuracy: 0.9766\n",
      "Epoch 00158: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0771 - accuracy: 0.9772 - val_loss: 0.1391 - val_accuracy: 0.9844\n",
      "Epoch 159/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9759 ETA: 0s - loss: 0.0818 - accuracy: 0.\n",
      "Epoch 00159: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0770 - accuracy: 0.9759 - val_loss: 0.1129 - val_accuracy: 0.9766\n",
      "Epoch 160/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9814\n",
      "Epoch 00160: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9811 - val_loss: 0.1308 - val_accuracy: 0.9792\n",
      "Epoch 161/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0849 - accuracy: 0.9721\n",
      "Epoch 00161: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0818 - accuracy: 0.9733 - val_loss: 0.1236 - val_accuracy: 0.9844\n",
      "Epoch 162/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0699 - accuracy: 0.9808\n",
      "Epoch 00162: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0687 - accuracy: 0.9805 - val_loss: 0.1340 - val_accuracy: 0.9818\n",
      "Epoch 163/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0792 - accuracy: 0.9775\n",
      "Epoch 00163: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0742 - accuracy: 0.9785 - val_loss: 0.1238 - val_accuracy: 0.9792\n",
      "Epoch 164/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0533 - accuracy: 0.9836\n",
      "Epoch 00164: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0592 - accuracy: 0.9824 - val_loss: 0.1256 - val_accuracy: 0.9792\n",
      "Epoch 165/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0765 - accuracy: 0.9785\n",
      "Epoch 00165: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 0.1263 - val_accuracy: 0.9818\n",
      "Epoch 166/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9847\n",
      "Epoch 00166: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0685 - accuracy: 0.9837 - val_loss: 0.1323 - val_accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0807 - accuracy: 0.9747\n",
      "Epoch 00167: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9739 - val_loss: 0.1163 - val_accuracy: 0.9766\n",
      "Epoch 168/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0908 - accuracy: 0.9722\n",
      "Epoch 00168: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0869 - accuracy: 0.9739 - val_loss: 0.1422 - val_accuracy: 0.9818\n",
      "Epoch 169/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0690 - accuracy: 0.9729\n",
      "Epoch 00169: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0667 - accuracy: 0.9739 - val_loss: 0.1258 - val_accuracy: 0.9740\n",
      "Epoch 170/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.0790 - accuracy: 0.9824\n",
      "Epoch 00170: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9837 - val_loss: 0.1398 - val_accuracy: 0.9818\n",
      "Epoch 171/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0915 - accuracy: 0.9721\n",
      "Epoch 00171: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0884 - accuracy: 0.9733 - val_loss: 0.1336 - val_accuracy: 0.9688\n",
      "Epoch 172/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 0.0963 - accuracy: 0.9766\n",
      "Epoch 00172: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0887 - accuracy: 0.9759 - val_loss: 0.0871 - val_accuracy: 0.9792\n",
      "Epoch 173/200\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.0842 - accuracy: 0.9744\n",
      "Epoch 00173: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0885 - accuracy: 0.9746 - val_loss: 0.1219 - val_accuracy: 0.9714\n",
      "Epoch 174/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0864 - accuracy: 0.9760\n",
      "Epoch 00174: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0854 - accuracy: 0.9752 - val_loss: 0.1313 - val_accuracy: 0.9818\n",
      "Epoch 175/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0684 - accuracy: 0.9803\n",
      "Epoch 00175: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.0667 - accuracy: 0.9811 - val_loss: 0.1258 - val_accuracy: 0.9844\n",
      "Epoch 176/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0955 - accuracy: 0.9721\n",
      "Epoch 00176: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9726 - val_loss: 0.1240 - val_accuracy: 0.9818\n",
      "Epoch 177/200\n",
      "37/48 [======================>.......] - ETA: 0s - loss: 0.0511 - accuracy: 0.9848\n",
      "Epoch 00177: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0511 - accuracy: 0.9857 - val_loss: 0.1057 - val_accuracy: 0.9844\n",
      "Epoch 178/200\n",
      "39/48 [=======================>......] - ETA: 0s - loss: 0.0812 - accuracy: 0.9792\n",
      "Epoch 00178: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0882 - accuracy: 0.9785 - val_loss: 0.1509 - val_accuracy: 0.9792\n",
      "Epoch 179/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0782 - accuracy: 0.9702\n",
      "Epoch 00179: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9700 - val_loss: 0.1601 - val_accuracy: 0.9688\n",
      "Epoch 180/200\n",
      "42/48 [=========================>....] - ETA: 0s - loss: 0.0633 - accuracy: 0.9821\n",
      "Epoch 00180: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0617 - accuracy: 0.9824 - val_loss: 0.1439 - val_accuracy: 0.9792\n",
      "Epoch 181/200\n",
      "36/48 [=====================>........] - ETA: 0s - loss: 0.0773 - accuracy: 0.9766\n",
      "Epoch 00181: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0711 - accuracy: 0.9779 - val_loss: 0.1535 - val_accuracy: 0.9766\n",
      "Epoch 182/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 0.0613 - accuracy: 0.9804\n",
      "Epoch 00182: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0560 - accuracy: 0.9824 - val_loss: 0.1781 - val_accuracy: 0.9661\n",
      "Epoch 183/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0565 - accuracy: 0.9817\n",
      "Epoch 00183: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9824 - val_loss: 0.1417 - val_accuracy: 0.9844\n",
      "Epoch 184/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0464 - accuracy: 0.9826\n",
      "Epoch 00184: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0421 - accuracy: 0.9844 - val_loss: 0.1671 - val_accuracy: 0.9688\n",
      "Epoch 185/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0531 - accuracy: 0.9836\n",
      "Epoch 00185: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.1635 - val_accuracy: 0.9818\n",
      "Epoch 186/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0624 - accuracy: 0.9861\n",
      "Epoch 00186: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0631 - accuracy: 0.9844 - val_loss: 0.1603 - val_accuracy: 0.9766\n",
      "Epoch 187/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 0.0803 - accuracy: 0.9762\n",
      "Epoch 00187: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9759 - val_loss: 0.1468 - val_accuracy: 0.9818\n",
      "Epoch 188/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.0820 - accuracy: 0.9771\n",
      "Epoch 00188: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9779 - val_loss: 0.1470 - val_accuracy: 0.9792\n",
      "Epoch 189/200\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1097 - accuracy: 0.9771\n",
      "Epoch 00189: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9759 - val_loss: 0.1049 - val_accuracy: 0.9792\n",
      "Epoch 190/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0724 - accuracy: 0.9803\n",
      "Epoch 00190: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9785 - val_loss: 0.1571 - val_accuracy: 0.9766\n",
      "Epoch 191/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9844\n",
      "Epoch 00191: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9844 - val_loss: 0.1178 - val_accuracy: 0.9792\n",
      "Epoch 192/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0676 - accuracy: 0.9837\n",
      "Epoch 00192: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.0739 - accuracy: 0.9824 - val_loss: 0.1687 - val_accuracy: 0.9844\n",
      "Epoch 193/200\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.0821 - accuracy: 0.9755\n",
      "Epoch 00193: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9752 - val_loss: 0.1329 - val_accuracy: 0.9818\n",
      "Epoch 194/200\n",
      "38/48 [======================>.......] - ETA: 0s - loss: 0.0933 - accuracy: 0.9745\n",
      "Epoch 00194: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0913 - accuracy: 0.9759 - val_loss: 0.1561 - val_accuracy: 0.9818\n",
      "Epoch 195/200\n",
      "43/48 [=========================>....] - ETA: 0s - loss: 0.0661 - accuracy: 0.9797\n",
      "Epoch 00195: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9798 - val_loss: 0.1620 - val_accuracy: 0.9844\n",
      "Epoch 196/200\n",
      "35/48 [====================>.........] - ETA: 0s - loss: 0.0500 - accuracy: 0.9848\n",
      "Epoch 00196: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9837 - val_loss: 0.1508 - val_accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "40/48 [========================>.....] - ETA: 0s - loss: 0.0631 - accuracy: 0.9805\n",
      "Epoch 00197: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9818 - val_loss: 0.1158 - val_accuracy: 0.9844\n",
      "Epoch 198/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9785\n",
      "Epoch 00198: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9785 - val_loss: 0.1284 - val_accuracy: 0.9818\n",
      "Epoch 199/200\n",
      "41/48 [========================>.....] - ETA: 0s - loss: 0.0695 - accuracy: 0.9771\n",
      "Epoch 00199: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9772 - val_loss: 0.1104 - val_accuracy: 0.9844\n",
      "Epoch 200/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9798\n",
      "Epoch 00200: val_loss did not improve from 0.08448\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9798 - val_loss: 0.1072 - val_accuracy: 0.9818\n",
      "Training completed in time:  0:00:56.044964\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 200\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp_with_sc.hdf5',verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349646b5",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba10906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import *\n",
    "# from tkinter import messagebox\n",
    "# from tkinter import Frame\n",
    "# from tkinter import ttk\n",
    "# import sounddevice as sound \n",
    "# from scipy.io.wavfile import write\n",
    "# import time\n",
    "# import wavio as wv\n",
    "# from IPython.display import Javascript, display\n",
    "# from ipywidgets import widgets    \n",
    "        \n",
    "# app = Tk()\n",
    "# app.title(\"Speaker Recognition System\")\n",
    "# # app.configure(bg = '#3c446b')\n",
    "# # app.configure(bg = '#000000')\n",
    "# app.configure(bg = '#111111')\n",
    "# app.resizable(False,False)\n",
    "\n",
    "# #center screen work\n",
    "# w = 500 # width for the Tk root\n",
    "# h = 510 # height for the Tk root\n",
    "\n",
    "# ws = app.winfo_screenwidth() # width of the screen\n",
    "# hs = app.winfo_screenheight() # height of the screen\n",
    "\n",
    "# # calculate x and y coordinates for the Tk root window\n",
    "# x = (ws/2) - (w/2)\n",
    "# y = (hs/2) - (h/2)\n",
    "\n",
    "# app.geometry('%dx%d+%d+%d' % (w, h, x, y))\n",
    "\n",
    "# #here the prediction button work happening        \n",
    "# def run_all():\n",
    "#     display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index(), IPython.notebook.ncells())'))\n",
    "#     app.after(6000, lambda: app.destroy())\n",
    "\n",
    "# #voice recording work\n",
    "# def Record():\n",
    "#     app.update()\n",
    "#     check = False\n",
    "#     try:\n",
    "#         sample_rate = 44100\n",
    "# #       oper wala freq is not librosa sample rate our model is trained on below sample rate\n",
    "# #         that why 44100 sahi nahi hai. \n",
    "# #         yeh wala correct hai\n",
    "# #         sample_rate = 22050\n",
    "#         my_temp = int(duration.get())\n",
    "#         check = True\n",
    "        \n",
    "#     except ValueError :\n",
    "#         check = False\n",
    "#         messagebox.showerror(\"ERROR\",\"enter the right value\")\n",
    "        \n",
    "#     if check == True:\n",
    "#         recording = sound.rec(my_temp*sample_rate, samplerate=sample_rate, channels=2)\n",
    "#         sound.wait()\n",
    "#         write(\"recording.wav\",sample_rate,recording)\n",
    "#         countdown.set('recording ended')\n",
    "#         messagebox.showinfo(\"Sucess\",\"voice is recorded\")\n",
    "        \n",
    "# def close_app():\n",
    "#     response=messagebox.askyesno('Exit','Are you sure you want to exit?')\n",
    "#     if response:\n",
    "#         app.destroy()\n",
    "    \n",
    "# # background image work   \n",
    "# load= PhotoImage(file='images\\\\backg1.png')\n",
    "# img = Label(app, image=load)\n",
    "# img.place(x=0, y=0)\n",
    "\n",
    "# #labelwork\n",
    "# Label(text=\"Voice Recognition System\",font=\"arial 28 bold\", bg='black',fg='orange').pack(padx=5,pady=5)\n",
    "\n",
    "# #messagebox for time\n",
    "# duration=StringVar()\n",
    "# entry=Entry(app,textvariable = duration,font= \"arial 30\",width=12).pack(pady=10)\n",
    "# Label(text=\"Enter record duration(in secs)\",font=\"arial 12\", bg='black',fg=\"white\").pack()\n",
    "\n",
    "# #button work\n",
    "# record_img= PhotoImage(file='images\\\\record.png')\n",
    "# record_btn=Button(app,image = record_img,bg='black',border=0,command=Record)\n",
    "# record_btn.place(x=60,y=170)\n",
    "\n",
    "# predict_img= PhotoImage(file='images\\\\prebtn.png')\n",
    "# predict_btn=Button(app, image = predict_img,bg='black', border=0,command= run_all)\n",
    "# predict_btn.place(x=270,y=170)\n",
    "\n",
    "# close_img=PhotoImage(file='images\\\\clsbtn.png')\n",
    "# close_btn=Button(app, image=close_img, bg='black',border=0,command=close_app)\n",
    "# close_btn.place(x=310,y=410)\n",
    "\n",
    "# countdown = StringVar()\n",
    "# Entry(app, textvariable = countdown, width = 13, font = 'Arial 20', bg='black',fg='orange').pack(pady=120)\n",
    "# countdown.set('start recording')\n",
    "\n",
    "# app.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14c13634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2889.998644516788\n"
     ]
    }
   ],
   "source": [
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bf599d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557.985370948461\n"
     ]
    }
   ],
   "source": [
    "print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4953a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=num_of_mfccs)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "        spectral_centroids = librosa.feature.spectral_centroid(audio_data, sr=sample_rate)[0]\n",
    "        spectral_centroids = np.mean(spectral_centroids.T,axis=0)\n",
    "        \n",
    "        F = []\n",
    "        temp_mfcc = mfccsscaled\n",
    "        temp_sc =((spectral_centroids - minimum)/ (maximum - minimum))\n",
    "        concat = np.hstack((temp_mfcc,temp_sc))\n",
    "        F.append(concat)\n",
    "        \n",
    "        F = np.array(F, dtype=np.float32) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None, None\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9382fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_feature(file_name)\n",
    "    print(type(prediction_feature) , \"predction feature type\")\n",
    "    print(\"prediction features :\")\n",
    "    print(prediction_feature)\n",
    "    \n",
    "    predicted_vector = np.argmax(model.predict(prediction_feature),axis=1)\n",
    "    print (\"predicted vector :\")\n",
    "    print(predicted_vector)\n",
    "\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(predicted_class)\n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n')\n",
    "    \n",
    "    \n",
    "    predicted_proba_vector = model.predict(prediction_feature)\n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )\n",
    "        \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f61115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter class number1\n",
      "custom_dataset\\audio\\unknown-test-data\\fold1\\1_21_1.wav\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14540/4187737205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpred_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# print_classname(filename)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "#unknown test data\n",
    "# filename = r\"recording.wav\"\n",
    "\n",
    "class_num = input(\"enter class number\")\n",
    "backslash = \"\\\\\"\n",
    "filename =r\"custom_dataset\\audio\\unknown-test-data\\fold\" +class_num +backslash  +class_num+\"_21_1.wav\"\n",
    "\n",
    "# filename = r\"C:\\Users\\user\\Desktop\\UNI\\semester 6\\updated ml prj\\Project Code\\cs182019_Final_Project\\live recored voices 18th feb\\wav\\babar.wav\"\n",
    "# filename = r\"C:\\Users\\user\\Desktop\\UNI\\semester 6\\updated ml prj\\Project Code\\cs182019_Final_Project\\library_recs\\laiba\\recording.wav\"\n",
    "print(filename)\n",
    "\n",
    "pred_cls = print_prediction(filename)\n",
    "print(pred_cls)\n",
    "# print_classname(filename)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce1526f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import *\n",
    "# from tkinter import messagebox\n",
    "# from tkinter import Frame\n",
    "# from tkinter import ttk\n",
    "# import sounddevice as sound \n",
    "# from scipy.io.wavfile import write\n",
    "# import time\n",
    "# import wavio as wv\n",
    "# from IPython.display import Javascript, display\n",
    "# from ipywidgets import widgets\n",
    "    \n",
    "# app = Tk()\n",
    "# app.geometry(\"500x500\")\n",
    "# app.title(\"Speaker Recognition System\")\n",
    "# app.configure(bg = '#3c446b')\n",
    "# # app.eval('tk::PlaceWindow . center')\n",
    "         \n",
    "# #center screen work\n",
    "# w = 500 # width for the Tk root\n",
    "# h = 510 # height for the Tk root\n",
    "\n",
    "# ws = app.winfo_screenwidth() # width of the screen\n",
    "# hs = app.winfo_screenheight() # height of the screen\n",
    "\n",
    "# # calculate x and y coordinates for the Tk root window\n",
    "# x = (ws/2) - (w/2)\n",
    "# y = (hs/2) - (h/2)\n",
    "\n",
    "# app.geometry('%dx%d+%d+%d' % (w, h, x, y))\n",
    "\n",
    "#  #background image work   \n",
    "# load= PhotoImage(file='images\\\\backg1.png')\n",
    "# img = Label(app, image=load)\n",
    "# img.place(x=0, y=0)\n",
    "\n",
    "# #labelwork\n",
    "# Label(text=\"Predicted Speaker\",font=\"arial 28 bold\", bg='black',fg='orange').pack(padx=5,pady=5)\n",
    "# Label(text = print_prediction(filename), font=\"arial 28 bold\", bg='black',fg='orange').pack(padx=5,pady=5)\n",
    "\n",
    "# app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07246cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
